{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb66ac4b",
   "metadata": {},
   "source": [
    "# Algorithm Project Proposal  \n",
    "**Team C.J.J.: Joshua Meyer, Chris Wong, Jeremy Orona**  \n",
    "**CPSC 322, Fall 2025**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Project Title\n",
    "**Predicting Bitcoin Price Direction Using Discretized Treasury and Sentiment Data**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Dataset Description\n",
    "\n",
    "### Source\n",
    "- **Primary Dataset:** [Kaggle â€” Bitcoin and US Treasury with Daily Sentiment](https://www.kaggle.com/datasets/jessearzate/bitcoin-and-us-treasury-with-daily-sentiment?select=bitcoin_sentiment_12012022_11082025.csv)\n",
    "- **Processed Dataset:** `bitcoin_sentiment_discretized.csv` (preprocessed version with discretized features)\n",
    "\n",
    "### Format\n",
    "- **File Type:** CSV (Comma-Separated Values)\n",
    "- **Encoding:** UTF-8\n",
    "\n",
    "### Contents\n",
    "The dataset contains daily financial and market data spanning **December 1, 2022 to November 2025**, with approximately **1,074 instances** and **19 predictor attributes** plus one target variable. The data has been preprocessed and discretized into categorical bins for classification purposes.\n",
    "\n",
    "**Key Data Components:**\n",
    "- **Bitcoin Trading Metrics:** Trading volume (discretized as VeryLow, Low, Medium, High, VeryHigh)\n",
    "- **US Treasury Indicators:** Multiple treasury security categories including:\n",
    "  - Treasury bills, bonds, notes, and floating rate notes (FRN)\n",
    "  - Treasury Inflation-Protected Securities (TIPS)\n",
    "  - Government account series and special purpose vehicles\n",
    "  - Federal financing bank data\n",
    "  - Total interest-bearing debt, marketable, and non-marketable securities\n",
    "- **Market Sentiment:** Weighted sentiment scores (discretized into categorical levels)\n",
    "- **Target Variable:** Binary price direction classification (Up/Down)\n",
    "\n",
    "### Dataset Size\n",
    "- **Instances:** ~1,074 daily observations\n",
    "- **Features:** 19 predictor attributes (all discretized categorical)\n",
    "- **Target Classes:** 2 (binary classification: \"Up\" or \"Down\")\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Attributes and Target\n",
    "\n",
    "### Predictor Attributes (19 Features)\n",
    "All attributes have been discretized into categorical bins for classification:\n",
    "\n",
    "1. **Volume** (VeryLow, Low, Medium, High, VeryHigh)\n",
    "2. **Federal Financing Bank** (Bank1, Bank2, Bank3, Bank4, Bank5)\n",
    "3. **Foreign Series** (0, 1)\n",
    "4. **Government Account Series** (VeryLow, Low, Medium, High, VeryHigh)\n",
    "5. **Government Account Series Inflation Securities** (VeryLow, Low, Medium, High, VeryHigh)\n",
    "6. **Special Purpose Vehicle** (VeryLow, Low, Medium, High, VeryHigh)\n",
    "7. **State and Local Government Series** (VeryLow, Low, Medium, High, VeryHigh)\n",
    "8. **Total Interest-Bearing Debt** (VeryLow, Low, Medium, High, VeryHigh)\n",
    "9. **Total Marketable** (VeryLow, Low, Medium, High, VeryHigh)\n",
    "10. **Total Non-Marketable** (VeryLow, Low, Medium, High, VeryHigh)\n",
    "11. **Treasury Bills** (VeryLow, Low, Medium, High, VeryHigh)\n",
    "12. **Treasury Bonds** (VeryLow, Low, Medium, High, VeryHigh)\n",
    "13. **Treasury Floating Rate Notes (FRN)** (VeryLow, Low, Medium, High, VeryHigh)\n",
    "14. **Treasury Inflation-Protected Securities (TIPS)** (VeryLow, Low, Medium, High, VeryHigh)\n",
    "15. **Treasury Notes** (VeryLow, Low, Medium, High, VeryHigh)\n",
    "16. **United States Savings Inflation Securities** (VeryLow, Low, Medium, High, VeryHigh)\n",
    "17. **United States Savings Securities** (VeryLow, Low, Medium, High, VeryHigh)\n",
    "18. **Weighted Sentiment** (VeryLow, Low, Medium, High, VeryHigh)\n",
    "\n",
    "### Target (Class Information)\n",
    "- **Target Variable:** `price_direction`\n",
    "- **Classification Type:** Binary classification\n",
    "- **Class Labels:** \n",
    "  - **\"Up\"**: Bitcoin closing price increased compared to the previous day\n",
    "  - **\"Down\"**: Bitcoin closing price decreased or remained the same compared to the previous day\n",
    "\n",
    "The target variable is derived by comparing each day's closing price with the previous day's closing price, creating a binary classification problem that predicts whether Bitcoin's price will move up or down based on treasury indicators and market sentiment.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Implementation / Technical Merit\n",
    "\n",
    "### Preprocessing Pipeline\n",
    "1. **Data Loading:** CSV parsing with proper encoding (UTF-8) and handling of categorical data\n",
    "2. **Feature Encoding:** One-Hot Encoding using scikit-learn's `OneHotEncoder` to convert categorical features into numerical format suitable for machine learning algorithms\n",
    "3. **Data Splitting:** Stratified train-test split (2/3 training, 1/3 testing) with `random_state=9` for reproducibility across all algorithms\n",
    "4. **Handling Unknown Categories:** OneHotEncoder configured with `handle_unknown=\"ignore\"` to gracefully handle unseen categorical values during prediction\n",
    "\n",
    "### Classification Algorithms\n",
    "We will implement and compare three distinct classification approaches:\n",
    "\n",
    "1. **Decision Tree Classifier (Baseline)**\n",
    "   - Implementation: scikit-learn's `DecisionTreeClassifier`\n",
    "   - Hyperparameters: Gini impurity criterion, max_depth=10, min_samples_split=2, min_samples_leaf=1\n",
    "   - Rationale: Provides interpretable baseline with decision rules that can be visualized\n",
    "\n",
    "2. **Random Forest Classifier (Custom Implementation)**\n",
    "   - Implementation: Custom `MyRandomForestClassifier` from course materials\n",
    "   - Hyperparameters: n_estimators=20, bootstrap=True, max_features determined by sqrt(n_features)\n",
    "   - Rationale: Demonstrates ensemble learning with bootstrap aggregation and random feature selection, showcasing custom implementation skills\n",
    "\n",
    "3. **K-Nearest Neighbors (KNN) Classifier**\n",
    "   - Implementation: scikit-learn's `KNeighborsClassifier`\n",
    "   - Hyperparameters: n_neighbors=5, weights=\"uniform\", metric=\"minkowski\" (Euclidean distance)\n",
    "   - Rationale: Instance-based learning approach that captures local patterns in the discretized feature space\n",
    "\n",
    "### Evaluation Methodology\n",
    "- **Consistent Evaluation:** All three algorithms use identical train-test splits (random_state=9, test_size=0.33, stratified)\n",
    "- **Performance Metrics:** Accuracy, precision, recall, F1-score, and classification reports\n",
    "- **Comparison Framework:** Direct comparison of model performance on identical test data to assess relative effectiveness\n",
    "\n",
    "### Technical Approach\n",
    "- **Pipeline Architecture:** scikit-learn Pipeline objects for preprocessing and classification integration\n",
    "- **Reproducibility:** Fixed random seeds ensure consistent results across algorithm comparisons\n",
    "- **Code Organization:** Modular design with separate demo scripts for each classifier, facilitating independent evaluation and comparison\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Anticipated Challenges\n",
    "\n",
    "### Data Preprocessing Challenges\n",
    "1. **Categorical Feature Encoding:** With 19 categorical features, one-hot encoding will create a high-dimensional feature space. This may lead to:\n",
    "   - Increased computational complexity\n",
    "   - Potential overfitting with limited training data\n",
    "   - Sparse feature matrices requiring efficient memory management\n",
    "\n",
    "2. **Class Distribution:** Binary classification may exhibit class imbalance if Bitcoin price movements are not evenly distributed between \"Up\" and \"Down\" classes. Mitigation strategies include:\n",
    "   - Stratified sampling in train-test splits (already implemented)\n",
    "   - Class weight balancing in algorithms that support it\n",
    "   - Evaluation metrics that account for imbalance (precision, recall, F1-score)\n",
    "\n",
    "3. **Temporal Dependencies:** Daily financial data contains inherent temporal structure, but we treat each day independently. This approach:\n",
    "   - Ignores potential autocorrelation in price movements\n",
    "   - May miss sequential patterns that could improve prediction\n",
    "   - Simplifies the problem but may limit model performance\n",
    "\n",
    "### Classification Challenges\n",
    "1. **High Dimensionality:** After one-hot encoding, the feature space expands significantly, potentially leading to:\n",
    "   - Curse of dimensionality effects, especially for KNN\n",
    "   - Increased risk of overfitting in decision trees\n",
    "   - Need for regularization or feature selection\n",
    "\n",
    "2. **Discretization Information Loss:** Converting continuous treasury and sentiment values to categorical bins may:\n",
    "   - Lose fine-grained information that could be predictive\n",
    "   - Introduce discretization bias depending on bin boundaries\n",
    "   - Reduce model sensitivity to subtle feature variations\n",
    "\n",
    "3. **Model Interpretability vs. Performance Trade-off:**\n",
    "   - Decision trees offer high interpretability but may underperform\n",
    "   - Random forests improve performance but reduce interpretability\n",
    "   - KNN provides no explicit feature importance insights\n",
    "\n",
    "### Feature Selection Considerations\n",
    "With 19 original features (expanding to many more after one-hot encoding), we will:\n",
    "- **Monitor Feature Importance:** Use embedded methods (decision tree feature importance, random forest feature importance) to identify most predictive features\n",
    "- **Evaluate Feature Redundancy:** Analyze correlation between discretized features to identify potential redundancies\n",
    "- **Consider Dimensionality Reduction:** If needed, explore PCA or feature selection techniques, though this may reduce interpretability\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Feature Selection Techniques\n",
    "\n",
    "Given the discretized categorical nature of our dataset and the expansion of features through one-hot encoding, we will employ the following feature selection strategies:\n",
    "\n",
    "### Embedded Methods (Primary Approach)\n",
    "1. **Decision Tree Feature Importance:** \n",
    "   - Extract feature importance scores from trained decision trees\n",
    "   - Identify features that contribute most to classification decisions\n",
    "   - Visualize feature importance rankings\n",
    "\n",
    "2. **Random Forest Feature Importance:**\n",
    "   - Aggregate feature importance across multiple trees in the forest\n",
    "   - More robust than single-tree importance due to ensemble averaging\n",
    "   - Identify features consistently important across bootstrap samples\n",
    "\n",
    "### Filter Methods (Secondary Analysis)\n",
    "1. **Correlation Analysis:**\n",
    "   - Examine correlations between one-hot encoded features and target variable\n",
    "   - Identify features with strong predictive relationships\n",
    "   - Note: Limited utility with categorical data, but can reveal patterns\n",
    "\n",
    "2. **Class Distribution Analysis:**\n",
    "   - Analyze how feature value distributions differ between \"Up\" and \"Down\" classes\n",
    "   - Identify features with distinct distributions across classes\n",
    "   - Use chi-square tests or similar statistical measures for categorical features\n",
    "\n",
    "### Dimensionality Considerations\n",
    "- **Current Approach:** Use all features with one-hot encoding to preserve information\n",
    "- **Future Consideration:** If model performance is poor or overfitting occurs, we may:\n",
    "  - Select top-k features based on importance scores\n",
    "  - Use recursive feature elimination\n",
    "  - Apply PCA (though this reduces interpretability with categorical data)\n",
    "\n",
    "### Evaluation Strategy\n",
    "- Compare model performance with full feature set vs. reduced feature sets\n",
    "- Balance between model complexity and performance\n",
    "- Maintain interpretability where possible, especially for decision trees\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Potential Impact of Results\n",
    "\n",
    "### Usefulness of Results\n",
    "\n",
    "**Financial Market Prediction:**\n",
    "- Our models can help identify relationships between US Treasury indicators, market sentiment, and Bitcoin price movements\n",
    "- Provides quantitative framework for testing hypotheses about macroeconomic factors influencing cryptocurrency markets\n",
    "- Demonstrates practical application of machine learning to financial time series prediction\n",
    "\n",
    "**Methodological Contributions:**\n",
    "- Compares three distinct classification paradigms (rule-based, ensemble, instance-based) on identical financial data\n",
    "- Evaluates effectiveness of discretization strategies for financial prediction\n",
    "- Provides reproducible baseline for future research on cryptocurrency price prediction\n",
    "\n",
    "**Educational Value:**\n",
    "- Showcases implementation of custom machine learning algorithms (Random Forest) alongside standard library implementations\n",
    "- Demonstrates proper evaluation methodology with consistent train-test splits\n",
    "- Illustrates challenges and solutions in preprocessing categorical financial data\n",
    "\n",
    "### Stakeholders\n",
    "\n",
    "**Primary Stakeholders:**\n",
    "1. **Cryptocurrency Investors and Traders:**\n",
    "   - Retail investors seeking data-driven insights for Bitcoin trading decisions\n",
    "   - Day traders looking for short-term price movement indicators\n",
    "   - Long-term holders interested in understanding market dynamics\n",
    "\n",
    "2. **Financial Institutions:**\n",
    "   - Hedge funds and investment firms developing quantitative trading strategies\n",
    "   - Banks and financial services companies assessing cryptocurrency market exposure\n",
    "   - Asset management companies evaluating Bitcoin as an investment asset class\n",
    "\n",
    "3. **Academic Researchers:**\n",
    "   - Researchers studying relationships between traditional financial markets and cryptocurrencies\n",
    "   - Economists analyzing macroeconomic factors affecting digital assets\n",
    "   - Data scientists developing financial prediction models\n",
    "\n",
    "**Secondary Stakeholders:**\n",
    "4. **Regulatory Bodies:**\n",
    "   - Financial regulators understanding market dynamics for policy development\n",
    "   - Government agencies monitoring cryptocurrency market stability\n",
    "\n",
    "5. **Technology Companies:**\n",
    "   - Fintech companies building trading platforms and prediction tools\n",
    "   - Blockchain companies analyzing market adoption patterns\n",
    "\n",
    "### Practical Applications\n",
    "- **Trading Strategy Development:** Models can inform algorithmic trading strategies\n",
    "- **Risk Management:** Understanding price movement patterns aids in portfolio risk assessment\n",
    "- **Market Analysis:** Provides quantitative framework for analyzing cryptocurrency market behavior\n",
    "- **Educational Tool:** Demonstrates machine learning applications in finance for educational purposes\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Citations\n",
    "\n",
    "### Dataset\n",
    "- **Kaggle Dataset:** \"Bitcoin and US Treasury with Daily Sentiment\" by Jesse Arzate\n",
    "  - URL: https://www.kaggle.com/datasets/jessearzate/bitcoin-and-us-treasury-with-daily-sentiment\n",
    "  - License: Check Kaggle dataset license terms\n",
    "  - Citation: Arzate, J. (2022-2025). Bitcoin and US Treasury with Daily Sentiment. Kaggle.\n",
    "\n",
    "### Software Libraries and Tools\n",
    "- **scikit-learn** (v1.0+): Pedregosa et al., \"Scikit-learn: Machine Learning in Python,\" JMLR 12, pp. 2825-2830, 2011.\n",
    "  - Documentation: https://scikit-learn.org/stable/\n",
    "  - Used for: DecisionTreeClassifier, KNeighborsClassifier, train_test_split, OneHotEncoder, Pipeline, evaluation metrics\n",
    "\n",
    "- **NumPy:** Harris et al., \"Array programming with NumPy,\" Nature 585, pp. 357-362, 2020.\n",
    "  - Documentation: https://numpy.org/doc/\n",
    "  - Used for: Numerical array operations and data manipulation\n",
    "\n",
    "- **Matplotlib:** Hunter, J. D., \"Matplotlib: A 2D graphics environment,\" Computing in Science & Engineering, vol. 9, no. 3, pp. 90-95, 2007.\n",
    "  - Documentation: https://matplotlib.org/\n",
    "  - Used for: Data visualization and decision tree plotting\n",
    "\n",
    "- **Python Standard Library:** csv module for data loading\n",
    "\n",
    "### Course Materials\n",
    "- **MyRandomForestClassifier:** Custom implementation provided in CPSC 322 course materials\n",
    "- **MyEvaluation Functions:** Stratified train-test split and evaluation functions from course materials\n",
    "- **Course Textbook and Lecture Notes:** Algorithm implementations and theoretical foundations from CPSC 322 course content\n",
    "\n",
    "### References and Documentation\n",
    "- **scikit-learn Documentation:**\n",
    "  - Decision Trees: https://scikit-learn.org/stable/modules/tree.html\n",
    "  - K-Nearest Neighbors: https://scikit-learn.org/stable/modules/neighbors.html\n",
    "  - Preprocessing: https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "  - Model Evaluation: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "- **Machine Learning Theory:**\n",
    "  - Breiman, L. (2001). \"Random Forests.\" Machine Learning, 45(1), 5-32.\n",
    "  - Cover, T., & Hart, P. (1967). \"Nearest neighbor pattern classification.\" IEEE Transactions on Information Theory, 13(1), 21-27.\n",
    "  - Quinlan, J. R. (1986). \"Induction of decision trees.\" Machine Learning, 1(1), 81-106.\n",
    "\n",
    "### Data Preprocessing\n",
    "- Discretization methodology and one-hot encoding techniques based on standard machine learning preprocessing practices as covered in CPSC 322 course materials.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
