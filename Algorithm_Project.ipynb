{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d43297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "# Import and reload mysklearn package modules\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyDecisionTreeClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "from mysklearn.myevaluation import (\n",
    "    stratified_kfold_split, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    binary_precision_score, \n",
    "    binary_recall_score, \n",
    "    binary_f1_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099db1d4",
   "metadata": {},
   "source": [
    "# Bitcoin Price Direction Prediction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b2dfc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This notebook demonstrates a complete machine learning pipeline for Bitcoin price direction prediction:\n",
    "\n",
    "1. **Data Preprocessing**: Load raw Bitcoin sentiment data, create binary price direction labels, normalize features, and discretize into categorical bins\n",
    "2. **Data Leakage Prevention**: Drop OHLC features (open, high, low, close) to ensure the model predicts legitimately using only volume, sentiment, and macroeconomic indicators\n",
    "3. **Random Forest Training**: Train multiple Random Forest instances (N=100 trees, M=3 best trees, F=8 features per split) with different random states for robust evaluation\n",
    "4. **Evaluation**: Assess performance using stratified train/test split (33% test size), confusion matrices, per-class metrics, and OOB scores across 5 independent runs\n",
    "5. **Hyperparameter Tuning**: Grid search over N, M, and F parameters to identify optimal configuration\n",
    "\n",
    "**Methodology:**\n",
    "- Dataset is balanced (~50/50 Up/Down)\n",
    "- All numeric features discretized into categorical bins (VeryLow to VeryHigh)\n",
    "- Random Forest uses stratified sampling to preserve class distribution\n",
    "- Bootstrap sampling with random feature selection for each tree\n",
    "- Features used: trading volume, sentiment analysis, and treasury/debt indicators\n",
    "\n",
    "**Data Integrity:**\n",
    "- No temporal leakage (OHLC prices removed)\n",
    "- Legitimate forecasting task (predict tomorrow using today's non-price data)\n",
    "- Model performance reflects true predictive capability\n",
    "- Baseline comparison: 50% accuracy (random guessing for binary classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f922f69",
   "metadata": {},
   "source": [
    "## Step 1: Load and Examine the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "307406b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape:\n",
      "  Rows: 1074\n",
      "  Columns: 28\n",
      "\n",
      "Column Headers:\n",
      "['Unnamed: 0', 'timestamp', 'open', 'high', 'low', 'close', 'volume', 'datetime_utc', 'merge_date', 'domestic_series', 'federal_financing_bank', 'foreign_series', 'government_account_series', 'government_account_series_inflation_securities', 'special_purpose_vehicle', 'state_and_local_government_series', 'total_interest-bearing_debt', 'total_marketable', 'total_non-marketable', 'treasury_bills', 'treasury_bonds', 'treasury_floating_rate_notes_(frn)', 'treasury_inflation-protected_securities_(tips)', 'treasury_notes', 'united_states_savings_inflation_securities', 'united_states_savings_securities', 'weighted_sentiment', 'sentiment_missing']\n",
      "\n",
      "First 5 Rows:\n",
      "   Unnamed: 0      timestamp      open      high       low     close  \\\n",
      "0           0  1669852800000  17165.44  17317.80  16855.00  16980.08   \n",
      "1           1  1669939200000  16980.07  17108.25  16791.02  17094.71   \n",
      "2           2  1670025600000  17094.25  17158.42  16863.58  16888.53   \n",
      "3           3  1670112000000  16889.17  17199.99  16882.86  17108.90   \n",
      "4           4  1670198400000  17108.90  17424.59  16865.22  16966.05   \n",
      "\n",
      "         volume               datetime_utc  merge_date  domestic_series  ...  \\\n",
      "0  31798.991518  2022-12-01 00:00:00+00:00  2022-12-01            7.577  ...   \n",
      "1  23096.436867  2022-12-02 00:00:00+00:00  2022-12-02            7.577  ...   \n",
      "2  14081.450672  2022-12-03 00:00:00+00:00  2022-12-03            7.577  ...   \n",
      "3  16961.108288  2022-12-04 00:00:00+00:00  2022-12-04            7.577  ...   \n",
      "4  33618.451090  2022-12-05 00:00:00+00:00  2022-12-05            7.577  ...   \n",
      "\n",
      "   total_non-marketable  treasury_bills  treasury_bonds  \\\n",
      "0                 2.135           3.456           3.012   \n",
      "1                 2.135           3.456           3.012   \n",
      "2                 2.135           3.456           3.012   \n",
      "3                 2.135           3.456           3.012   \n",
      "4                 2.135           3.456           3.012   \n",
      "\n",
      "   treasury_floating_rate_notes_(frn)  \\\n",
      "0                               4.104   \n",
      "1                               4.104   \n",
      "2                               4.104   \n",
      "3                               4.104   \n",
      "4                               4.104   \n",
      "\n",
      "   treasury_inflation-protected_securities_(tips)  treasury_notes  \\\n",
      "0                                           0.487            1.68   \n",
      "1                                           0.487            1.68   \n",
      "2                                           0.487            1.68   \n",
      "3                                           0.487            1.68   \n",
      "4                                           0.487            1.68   \n",
      "\n",
      "   united_states_savings_inflation_securities  \\\n",
      "0                                      10.148   \n",
      "1                                      10.148   \n",
      "2                                      10.148   \n",
      "3                                      10.148   \n",
      "4                                      10.148   \n",
      "\n",
      "   united_states_savings_securities  weighted_sentiment  sentiment_missing  \n",
      "0                             2.694            0.408548                  0  \n",
      "1                             2.694            0.136171                  0  \n",
      "2                             2.694           -0.347766                  0  \n",
      "3                             2.694            0.502235                  0  \n",
      "4                             2.694            0.326272                  0  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the bitcoin sentiment dataset\n",
    "df = pd.read_csv('input_data/bitcoin_sentiment.csv')\n",
    "\n",
    "# Print dataset shape\n",
    "print(\"Dataset Shape:\")\n",
    "print(f\"  Rows: {df.shape[0]}\")\n",
    "print(f\"  Columns: {df.shape[1]}\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Print headers (column names)\n",
    "print(\"Column Headers:\")\n",
    "print(df.columns.tolist())\n",
    "print()\n",
    "\n",
    "# Print first few rows\n",
    "print(\"First 5 Rows:\")\n",
    "print(df.head())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343a56ce",
   "metadata": {},
   "source": [
    "## Inspect class/label distribution \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b864e30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution ('close'):\n",
      "0        16980.08\n",
      "1        17094.71\n",
      "2        16888.53\n",
      "3        17108.90\n",
      "4        16966.05\n",
      "          ...    \n",
      "1069    101468.15\n",
      "1070    103869.00\n",
      "1071    101290.50\n",
      "1072    103284.27\n",
      "1073    102249.20\n",
      "Name: close, Length: 1074, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print label distribution \n",
    "label_column = df.columns[5]\n",
    "print(f\"Label Distribution ('{label_column}'):\")\n",
    "print(df[label_column])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd19d4b3",
   "metadata": {},
   "source": [
    "## Inspect weight_sentiment and other features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ec9188f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Sentiment Statistics:\n",
      "count    1074.000000\n",
      "mean        0.347973\n",
      "std         0.274657\n",
      "min        -0.749771\n",
      "25%         0.171151\n",
      "50%         0.376796\n",
      "75%         0.540075\n",
      "max         0.952912\n",
      "Name: weighted_sentiment, dtype: float64\n",
      "\n",
      "No missing values found in the dataset.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Checking for rows where 'sentiment_missing' != 0:\n",
      "\n",
      "No rows with sentiment_missing != 0 found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examine the weighted_sentiment column more closely\n",
    "print(\"Weighted Sentiment Statistics:\")\n",
    "print(df['weighted_sentiment'].describe())\n",
    "print()\n",
    "\n",
    "# Check for missing values\n",
    "if df.isnull().values.any():\n",
    "    print(\"Missing Values per Column:\")\n",
    "    print(df.isnull().sum())\n",
    "else:\n",
    "    print(\"No missing values found in the dataset.\")\n",
    "print()\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print()\n",
    "\n",
    "# Check sentiment_missing column values for anything other than zero\n",
    "print(\"Checking for rows where 'sentiment_missing' != 0:\")\n",
    "print()\n",
    "if (df['sentiment_missing'] != 0).any():\n",
    "    print(df[df['sentiment_missing'] != 0])\n",
    "else:\n",
    "    print(\"No rows with sentiment_missing != 0 found.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5505a3",
   "metadata": {},
   "source": [
    "## Inspect weighted_sentiment distribution (exploratory analysis only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e219aa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Label Distribution:(for weighted_sentiment)\n",
      "sentiment_label\n",
      "Positive            949\n",
      "Negative/Neutral    125\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label Proportions:\n",
      "sentiment_label\n",
      "Positive            0.883613\n",
      "Negative/Neutral    0.116387\n",
      "Name: proportion, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create binary classification label from weighted_sentiment for exploratory analysis\n",
    "# NOTE: This is NOT used for model training - only for understanding sentiment distribution\n",
    "# Positive sentiment (>0) vs Negative/Neutral sentiment (<=0)\n",
    "df['sentiment_label'] = df['weighted_sentiment'].apply(lambda x: 'Positive' if x > 0 else 'Negative/Neutral')\n",
    "\n",
    "print(\"Classification Label Distribution (for weighted_sentiment):\")\n",
    "print(df['sentiment_label'].value_counts())\n",
    "print()\n",
    "print(\"Label Proportions:\")\n",
    "print(df['sentiment_label'].value_counts(normalize=True))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ffb35",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7788d55a",
   "metadata": {},
   "source": [
    "## Step 2: Create Classification Label (Price Direction)\n",
    "\n",
    "- Convert continuous `close` price into a binary classification target by comparing each day's closing price with the previous day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d22b012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discretized Label Distribution (price_direction):\n",
      "price_direction\n",
      "Up      543\n",
      "Down    530\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label Proportions:\n",
      "price_direction\n",
      "Up      0.506058\n",
      "Down    0.493942\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Total instances after discretization: 1073\n",
      "(Original: 1074, Removed first row: 1)\n"
     ]
    }
   ],
   "source": [
    "# Reload the original dataset\n",
    "df_original = pd.read_csv('input_data/bitcoin_sentiment.csv')\n",
    "\n",
    "# Create the discretized label by comparing close with previous day's close\n",
    "# First row will be dropped since there's no previous day\n",
    "df_original['price_direction'] = 'Down'  # Default value\n",
    "\n",
    "# Compare current close with previous close\n",
    "for i in range(1, len(df_original)):\n",
    "    if df_original.loc[i, 'close'] > df_original.loc[i-1, 'close']:\n",
    "        df_original.loc[i, 'price_direction'] = 'Up'\n",
    "    else:\n",
    "        df_original.loc[i, 'price_direction'] = 'Down'\n",
    "\n",
    "# Remove the first row (no previous day to compare)\n",
    "df_discretized = df_original.iloc[1:].copy()\n",
    "df_discretized = df_discretized.reset_index(drop=True)\n",
    "\n",
    "print(\"Discretized Label Distribution (price_direction):\")\n",
    "print(df_discretized['price_direction'].value_counts())\n",
    "print()\n",
    "print(\"Label Proportions:\")\n",
    "print(df_discretized['price_direction'].value_counts(normalize=True))\n",
    "print()\n",
    "print(f\"Total instances after discretization: {len(df_discretized)}\")\n",
    "print(f\"(Original: {len(df_original)}, Removed first row: 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5e7ba5",
   "metadata": {},
   "source": [
    "## Step 3: Drop Unnecessary Features\n",
    "\n",
    "Remove temporal columns, identifiers, constant features, and **OHLC price features** to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dbde3087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after dropping unnecessary columns:\n",
      "  Rows: 1073\n",
      "  Columns: 19\n",
      "  Dropped: ['Unnamed: 0', 'timestamp', 'datetime_utc', 'merge_date', 'sentiment_missing', 'domestic_series', 'open', 'high', 'low', 'close']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns (temporal, IDs, constant features, and OHLC to prevent leakage)\n",
    "columns_to_drop = [\n",
    "    'Unnamed: 0', \n",
    "    'timestamp', \n",
    "    'datetime_utc', \n",
    "    'merge_date', \n",
    "    'sentiment_missing', \n",
    "    'domestic_series',\n",
    "    'open',   \n",
    "    'high', \n",
    "    'low',   \n",
    "    'close'   \n",
    "]\n",
    "df_clean = df_discretized.drop(columns=columns_to_drop)\n",
    "\n",
    "print(\"Dataset after dropping unnecessary columns:\")\n",
    "print(f\"  Rows: {df_clean.shape[0]}\")\n",
    "print(f\"  Columns: {df_clean.shape[1]}\")\n",
    "print(f\"  Dropped: {columns_to_drop}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcabbc58",
   "metadata": {},
   "source": [
    "## Step 4: Normalize Numeric Features\n",
    "\n",
    "Apply z-score normalization to scale all numeric features to mean=0 and std=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f15787d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 numeric features to normalize\n",
      "\n",
      "Feature ranges before normalization:\n",
      "  volume: [1227.77, 65575.10]\n",
      "  federal_financing_bank: [2.39, 2.58]\n",
      "  foreign_series: [0.00, 7.31]\n",
      "  government_account_series: [2.13, 3.17]\n",
      "  government_account_series_inflation_securities: [0.99, 1.31]\n",
      "  special_purpose_vehicle: [2.89, 4.17]\n",
      "  state_and_local_government_series: [1.81, 3.85]\n",
      "  total_interest-bearing_debt: [2.22, 3.37]\n",
      "  total_marketable: [2.24, 3.42]\n",
      "  total_non-marketable: [2.13, 3.19]\n",
      "  treasury_bills: [3.46, 5.45]\n",
      "  treasury_bonds: [3.01, 3.34]\n",
      "  treasury_floating_rate_notes_(frn): [3.90, 5.54]\n",
      "  treasury_inflation-protected_securities_(tips): [0.49, 0.96]\n",
      "  treasury_notes: [1.68, 3.12]\n",
      "  united_states_savings_inflation_securities: [3.08, 10.15]\n",
      "  united_states_savings_securities: [2.69, 3.49]\n",
      "  weighted_sentiment: [-0.75, 0.95]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify numeric columns (exclude the label column 'price_direction')\n",
    "numeric_columns = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Found {len(numeric_columns)} numeric features to normalize\")\n",
    "print()\n",
    "\n",
    "# Check the scale of numeric features before normalization\n",
    "print(\"Feature ranges before normalization:\")\n",
    "for col in numeric_columns[:18]:\n",
    "    print(f\"  {col}: [{df_clean[col].min():.2f}, {df_clean[col].max():.2f}]\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a22575",
   "metadata": {},
   "source": [
    "### 4.1 Apply Z-Score Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f0f62522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features standardized (z-score normalization)\n",
      "\n",
      "Feature statistics after normalization:\n",
      "      volume  federal_financing_bank  foreign_series  \\\n",
      "mean    -0.0                     0.0             0.0   \n",
      "std      1.0                     1.0             1.0   \n",
      "\n",
      "      government_account_series  \\\n",
      "mean                        0.0   \n",
      "std                         1.0   \n",
      "\n",
      "      government_account_series_inflation_securities  special_purpose_vehicle  \\\n",
      "mean                                             0.0                     -0.0   \n",
      "std                                              1.0                      1.0   \n",
      "\n",
      "      state_and_local_government_series  total_interest-bearing_debt  \\\n",
      "mean                                0.0                         -0.0   \n",
      "std                                 1.0                          1.0   \n",
      "\n",
      "      total_marketable  total_non-marketable  treasury_bills  treasury_bonds  \\\n",
      "mean              -0.0                  -0.0             0.0             0.0   \n",
      "std                1.0                   1.0             1.0             1.0   \n",
      "\n",
      "      treasury_floating_rate_notes_(frn)  \\\n",
      "mean                                 0.0   \n",
      "std                                  1.0   \n",
      "\n",
      "      treasury_inflation-protected_securities_(tips)  treasury_notes  \\\n",
      "mean                                            -0.0             0.0   \n",
      "std                                              1.0             1.0   \n",
      "\n",
      "      united_states_savings_inflation_securities  \\\n",
      "mean                                         0.0   \n",
      "std                                          1.0   \n",
      "\n",
      "      united_states_savings_securities  weighted_sentiment  \n",
      "mean                               0.0                 0.0  \n",
      "std                                1.0                 1.0  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# z-score normalization: (x - mean) / std\n",
    "# This transforms each feature to have mean=0 and std=1\n",
    "\n",
    "df_normalized = df_clean.copy()\n",
    "\n",
    "for col in numeric_columns:\n",
    "    mean = df_normalized[col].mean()\n",
    "    std = df_normalized[col].std()\n",
    "    \n",
    "    # Avoid division by zero for constant columns\n",
    "    if std > 0:\n",
    "        df_normalized[col] = (df_normalized[col] - mean) / std\n",
    "    else:\n",
    "        print(f\"Warning: {col} has std=0, skipping normalization\")\n",
    "\n",
    "print(\"Numeric features standardized (z-score normalization)\")\n",
    "print()\n",
    "\n",
    "# Verify normalization\n",
    "print(\"Feature statistics after normalization:\")\n",
    "print(df_normalized[numeric_columns].describe().loc[['mean', 'std']].round(6))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44cfe64",
   "metadata": {},
   "source": [
    "## Step 5: Discretize Features into Categorical Bins\n",
    "\n",
    "Convert normalized numeric features into categorical bins for entropy-based decision tree classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c6df99d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discretizing numeric features into 5 categorical bins...\n",
      "Bins: VeryLow, Low, Medium, High, VeryHigh\n",
      "\n",
      "Warning: Could not discretize 'federal_financing_bank': Bin labels must be one fewer than the number of bin edges\n",
      "Warning: Could not discretize 'foreign_series': Bin labels must be one fewer than the number of bin edges\n",
      "Skipped 2 columns: ['federal_financing_bank', 'foreign_series']\n"
     ]
    }
   ],
   "source": [
    "# Discretize normalized numeric features into categorical bins\n",
    "# Using quantile-based binning (equal frequency bins)\n",
    "\n",
    "df_discretized_final = df_normalized.copy()\n",
    "\n",
    "# Define binning strategy: convert normalized values to 5 categories\n",
    "# Since normalized data has mean=0, std=1, we can use standard deviations as boundaries\n",
    "def discretize_normalized_feature(series, n_bins=5):\n",
    "    \"\"\"\n",
    "    Discretize a normalized feature into categorical bins.\n",
    "    Uses quantile-based binning for equal frequency distribution.\n",
    "    \"\"\"\n",
    "    # Use pandas qcut for quantile-based binning\n",
    "    bins = pd.qcut(series, q=n_bins, labels=['VeryLow', 'Low', 'Medium', 'High', 'VeryHigh'], duplicates='drop')\n",
    "    return bins\n",
    "\n",
    "print(\"Discretizing numeric features into 5 categorical bins...\")\n",
    "print(\"Bins: VeryLow, Low, Medium, High, VeryHigh\")\n",
    "print()\n",
    "\n",
    "# Track which columns get discretized\n",
    "discretized_cols = []\n",
    "skipped_cols = []\n",
    "\n",
    "for col in numeric_columns:\n",
    "    try:\n",
    "        df_discretized_final[col] = discretize_normalized_feature(df_discretized_final[col])\n",
    "        discretized_cols.append(col)\n",
    "    except Exception as e:\n",
    "        # Some columns might have too few unique values to discretize\n",
    "        print(f\"Warning: Could not discretize '{col}': {e}\")\n",
    "        skipped_cols.append(col)\n",
    "\n",
    "\n",
    "print(f\"Skipped {len(skipped_cols)} columns: {skipped_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03af641",
   "metadata": {},
   "source": [
    "### 5.1 Handle Columns with Too Few Unique Values for Quantile Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "731d11d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling skipped columns with manual discretization:\n",
      "  federal_financing_bank: 5 unique values\n",
      "    Direct mapping: 5 unique values → Bank1-Bank5\n",
      "    Distribution: {'Bank1': 131, 'Bank2': 365, 'Bank3': 366, 'Bank4': 181, 'Bank5': 30}\n",
      "  foreign_series: 2 unique values\n"
     ]
    }
   ],
   "source": [
    "# Handle columns that couldn't be discretized (too few unique values)\n",
    "# Convert them to categorical based on their actual unique values\n",
    "\n",
    "if skipped_cols:\n",
    "    print(\"Handling skipped columns with manual discretization:\")\n",
    "    for col in skipped_cols:\n",
    "        unique_vals = df_discretized_final[col].nunique()\n",
    "        print(f\"  {col}: {unique_vals} unique values\")\n",
    "        \n",
    "        # Apply custom discretization rules based on column name\n",
    "        if col == 'federal_financing_bank':\n",
    "            # Map 5 unique values directly to Bank1-Bank5 (preserves ordinality)\n",
    "            sorted_unique = sorted(df_normalized[col].unique())\n",
    "            bank_mapping = {val: f'Bank{i+1}' for i, val in enumerate(sorted_unique)}\n",
    "            df_discretized_final[col] = df_normalized[col].map(bank_mapping)\n",
    "            print(f\"    Direct mapping: {len(sorted_unique)} unique values → Bank1-Bank{len(sorted_unique)}\")\n",
    "            print(f\"    Distribution: {df_discretized_final[col].value_counts().sort_index().to_dict()}\")\n",
    "            \n",
    "        elif col == 'foreign_series':\n",
    "            # Bin into 0 or 1 (binary) using the original normalized data\n",
    "            df_discretized_final[col] = pd.cut(df_normalized[col], bins=2, labels=['0', '1'])\n",
    "            \n",
    "        elif unique_vals == 1:\n",
    "            # Constant feature\n",
    "            df_discretized_final[col] = 'Constant'\n",
    "            \n",
    "        elif unique_vals == 2:\n",
    "            df_discretized_final[col] = pd.cut(df_normalized[col], bins=2, labels=['Low', 'High'])\n",
    "            \n",
    "        elif unique_vals <= 5:\n",
    "            # For 3-5 unique values, map directly to preserve ordinal structure\n",
    "            sorted_unique = sorted(df_normalized[col].unique())\n",
    "            level_mapping = {val: f'Level_{i}' for i, val in enumerate(sorted_unique)}\n",
    "            df_discretized_final[col] = df_normalized[col].map(level_mapping)\n",
    "            \n",
    "        else:\n",
    "            # Default to 5 bins for columns with more unique values\n",
    "            df_discretized_final[col] = pd.qcut(df_normalized[col], q=5, \n",
    "                                                 labels=['VeryLow', 'Low', 'Medium', 'High', 'VeryHigh'], \n",
    "                                                 duplicates='drop')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "72ed849e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All categorical features converted to 'category' dtype\n",
      "\n",
      "Sample of discretized features:\n",
      "     volume federal_financing_bank foreign_series government_account_series  \\\n",
      "0  VeryHigh                  Bank4              1                   VeryLow   \n",
      "1      High                  Bank4              1                   VeryLow   \n",
      "2      High                  Bank4              1                   VeryLow   \n",
      "3  VeryHigh                  Bank4              1                   VeryLow   \n",
      "4  VeryHigh                  Bank4              1                   VeryLow   \n",
      "5  VeryHigh                  Bank4              1                   VeryLow   \n",
      "6  VeryHigh                  Bank4              1                   VeryLow   \n",
      "7  VeryHigh                  Bank4              1                   VeryLow   \n",
      "8       Low                  Bank4              1                   VeryLow   \n",
      "9    Medium                  Bank4              1                   VeryLow   \n",
      "\n",
      "  government_account_series_inflation_securities special_purpose_vehicle  \\\n",
      "0                                        VeryLow                     Low   \n",
      "1                                        VeryLow                     Low   \n",
      "2                                        VeryLow                     Low   \n",
      "3                                        VeryLow                     Low   \n",
      "4                                        VeryLow                     Low   \n",
      "5                                        VeryLow                     Low   \n",
      "6                                        VeryLow                     Low   \n",
      "7                                        VeryLow                     Low   \n",
      "8                                        VeryLow                     Low   \n",
      "9                                        VeryLow                     Low   \n",
      "\n",
      "  state_and_local_government_series total_interest-bearing_debt  \\\n",
      "0                           VeryLow                     VeryLow   \n",
      "1                           VeryLow                     VeryLow   \n",
      "2                           VeryLow                     VeryLow   \n",
      "3                           VeryLow                     VeryLow   \n",
      "4                           VeryLow                     VeryLow   \n",
      "5                           VeryLow                     VeryLow   \n",
      "6                           VeryLow                     VeryLow   \n",
      "7                           VeryLow                     VeryLow   \n",
      "8                           VeryLow                     VeryLow   \n",
      "9                           VeryLow                     VeryLow   \n",
      "\n",
      "  total_marketable total_non-marketable treasury_bills treasury_bonds  \\\n",
      "0          VeryLow              VeryLow        VeryLow        VeryLow   \n",
      "1          VeryLow              VeryLow        VeryLow        VeryLow   \n",
      "2          VeryLow              VeryLow        VeryLow        VeryLow   \n",
      "3          VeryLow              VeryLow        VeryLow        VeryLow   \n",
      "4          VeryLow              VeryLow        VeryLow        VeryLow   \n",
      "5          VeryLow              VeryLow        VeryLow        VeryLow   \n",
      "6          VeryLow              VeryLow        VeryLow        VeryLow   \n",
      "7          VeryLow              VeryLow        VeryLow        VeryLow   \n",
      "8          VeryLow              VeryLow        VeryLow        VeryLow   \n",
      "9          VeryLow              VeryLow        VeryLow        VeryLow   \n",
      "\n",
      "  treasury_floating_rate_notes_(frn)  \\\n",
      "0                            VeryLow   \n",
      "1                            VeryLow   \n",
      "2                            VeryLow   \n",
      "3                            VeryLow   \n",
      "4                            VeryLow   \n",
      "5                            VeryLow   \n",
      "6                            VeryLow   \n",
      "7                            VeryLow   \n",
      "8                            VeryLow   \n",
      "9                            VeryLow   \n",
      "\n",
      "  treasury_inflation-protected_securities_(tips) treasury_notes  \\\n",
      "0                                        VeryLow        VeryLow   \n",
      "1                                        VeryLow        VeryLow   \n",
      "2                                        VeryLow        VeryLow   \n",
      "3                                        VeryLow        VeryLow   \n",
      "4                                        VeryLow        VeryLow   \n",
      "5                                        VeryLow        VeryLow   \n",
      "6                                        VeryLow        VeryLow   \n",
      "7                                        VeryLow        VeryLow   \n",
      "8                                        VeryLow        VeryLow   \n",
      "9                                        VeryLow        VeryLow   \n",
      "\n",
      "  united_states_savings_inflation_securities united_states_savings_securities  \\\n",
      "0                                   VeryHigh                          VeryLow   \n",
      "1                                   VeryHigh                          VeryLow   \n",
      "2                                   VeryHigh                          VeryLow   \n",
      "3                                   VeryHigh                          VeryLow   \n",
      "4                                   VeryHigh                          VeryLow   \n",
      "5                                   VeryHigh                          VeryLow   \n",
      "6                                   VeryHigh                          VeryLow   \n",
      "7                                   VeryHigh                          VeryLow   \n",
      "8                                   VeryHigh                          VeryLow   \n",
      "9                                   VeryHigh                          VeryLow   \n",
      "\n",
      "  weighted_sentiment price_direction  \n",
      "0                Low              Up  \n",
      "1            VeryLow            Down  \n",
      "2               High              Up  \n",
      "3             Medium            Down  \n",
      "4            VeryLow              Up  \n",
      "5             Medium            Down  \n",
      "6            VeryLow              Up  \n",
      "7           VeryHigh            Down  \n",
      "8                Low            Down  \n",
      "9             Medium            Down  \n",
      "\n",
      "\n",
      "Distribution of discretized 'volume':\n",
      "volume\n",
      "VeryLow     215\n",
      "Low         214\n",
      "Medium      215\n",
      "High        214\n",
      "VeryHigh    215\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribution of discretized 'weighted_sentiment':\n",
      "weighted_sentiment\n",
      "VeryLow     215\n",
      "Low         214\n",
      "Medium      215\n",
      "High        214\n",
      "VeryHigh    215\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribution of discretized 'federal_financing_bank':\n",
      "federal_financing_bank\n",
      "Bank1    131\n",
      "Bank2    365\n",
      "Bank3    366\n",
      "Bank4    181\n",
      "Bank5     30\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribution of discretized 'foreign_series':\n",
      "foreign_series\n",
      "0    953\n",
      "1    120\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data types after discretization:\n",
      "volume                                            category\n",
      "federal_financing_bank                            category\n",
      "foreign_series                                    category\n",
      "government_account_series                         category\n",
      "government_account_series_inflation_securities    category\n",
      "special_purpose_vehicle                           category\n",
      "state_and_local_government_series                 category\n",
      "total_interest-bearing_debt                       category\n",
      "total_marketable                                  category\n",
      "total_non-marketable                              category\n",
      "treasury_bills                                    category\n",
      "treasury_bonds                                    category\n",
      "treasury_floating_rate_notes_(frn)                category\n",
      "treasury_inflation-protected_securities_(tips)    category\n",
      "treasury_notes                                    category\n",
      "united_states_savings_inflation_securities        category\n",
      "united_states_savings_securities                  category\n",
      "weighted_sentiment                                category\n",
      "price_direction                                   category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert all object columns to category dtype for better memory efficiency\n",
    "for col in df_discretized_final.columns:\n",
    "    if df_discretized_final[col].dtype == 'object':\n",
    "        df_discretized_final[col] = df_discretized_final[col].astype('category')\n",
    "\n",
    "print(\"All categorical features converted to 'category' dtype\")\n",
    "print()\n",
    "\n",
    "# Verify discretization - check value distributions\n",
    "print(\"Sample of discretized features:\")\n",
    "print(df_discretized_final.head(10))\n",
    "print()\n",
    "\n",
    "# Show distribution for a few key features (updated - no OHLC features)\n",
    "sample_features = ['volume', 'weighted_sentiment', 'federal_financing_bank', 'foreign_series']\n",
    "for feature in sample_features:\n",
    "    if feature in df_discretized_final.columns:\n",
    "        print(f\"\\nDistribution of discretized '{feature}':\")\n",
    "        print(df_discretized_final[feature].value_counts().sort_index())\n",
    "\n",
    "print()\n",
    "print(\"Data types after discretization:\")\n",
    "print(df_discretized_final.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9910ca",
   "metadata": {},
   "source": [
    "## Step 6: Save Preprocessed Dataset\n",
    "\n",
    "Export the final preprocessed dataset ready for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a1cd84d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Final preprocessed dataset saved to: input_data/bitcoin_sentiment_discretized.csv\n",
      "  Total instances: 1073\n",
      "  Total features: 18\n",
      "  Label column: 'price_direction'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save the final preprocessed dataset (ready for classification)\n",
    "output_file_final = 'input_data/bitcoin_sentiment_discretized.csv'\n",
    "df_discretized_final.to_csv(output_file_final, index=False)\n",
    "\n",
    "print(f\"✓ Final preprocessed dataset saved to: {output_file_final}\")\n",
    "print(f\"  Total instances: {len(df_discretized_final)}\")\n",
    "print(f\"  Total features: {len(df_discretized_final.columns) - 1}\")\n",
    "print(f\"  Label column: 'price_direction'\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e588a",
   "metadata": {},
   "source": [
    "## Preprocessing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2cc01550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Pipeline Summary:\n",
      "  1. Loaded raw Bitcoin sentiment dataset\n",
      "  2. Created binary classification label 'price_direction' (Up/Down) from daily close price changes\n",
      "  3. Dropped 10 unnecessary columns (temporal, IDs, constants, OHLC prices to prevent leakage)\n",
      "  4. Normalized 18 numeric features using z-score normalization (mean=0, std=1)\n",
      "  5. Discretized features into categorical bins using quantile-based binning:\n",
      "     - Standard features: VeryLow, Low, Medium, High, VeryHigh (5 equal-frequency bins)\n",
      "     - Special handling for columns with too few unique values\n",
      "  6. Converted all features to categorical dtype for memory efficiency\n",
      "  7. Final dataset: 1073 instances, 18 features\n",
      "  8. Label distribution: {'Up': 543, 'Down': 530}\n",
      "  9. Dataset ready for MyRandomForestClassifier training\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing Pipeline Summary:\")\n",
    "print(f\"  1. Loaded raw Bitcoin sentiment dataset\")\n",
    "print(f\"  2. Created binary classification label 'price_direction' (Up/Down) from daily close price changes\")\n",
    "print(f\"  3. Dropped {len(columns_to_drop)} unnecessary columns (temporal, IDs, constants, OHLC prices to prevent leakage)\")\n",
    "print(f\"  4. Normalized {len(numeric_columns)} numeric features using z-score normalization (mean=0, std=1)\")\n",
    "print(f\"  5. Discretized features into categorical bins using quantile-based binning:\")\n",
    "print(f\"     - Standard features: VeryLow, Low, Medium, High, VeryHigh (5 equal-frequency bins)\")\n",
    "print(f\"     - Special handling for columns with too few unique values\")\n",
    "print(f\"  6. Converted all features to categorical dtype for memory efficiency\")\n",
    "print(f\"  7. Final dataset: {len(df_discretized_final)} instances, {len(df_discretized_final.columns) - 1} features\")\n",
    "print(f\"  8. Label distribution: {df_discretized_final['price_direction'].value_counts().to_dict()}\")\n",
    "print(f\"  9. Dataset ready for MyRandomForestClassifier training\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6981575d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a142c613",
   "metadata": {},
   "source": [
    "## Step 1: Load Preprocessed Data for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab2ad638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Dataset Loaded:\n",
      "  Shape: (1073, 19)\n",
      "  Features: 18\n",
      "  Instances: 1073\n",
      "\n",
      "First 5 rows:\n",
      "     volume federal_financing_bank  foreign_series government_account_series  \\\n",
      "0  VeryHigh                  Bank4               1                   VeryLow   \n",
      "1      High                  Bank4               1                   VeryLow   \n",
      "2      High                  Bank4               1                   VeryLow   \n",
      "3  VeryHigh                  Bank4               1                   VeryLow   \n",
      "4  VeryHigh                  Bank4               1                   VeryLow   \n",
      "\n",
      "  government_account_series_inflation_securities special_purpose_vehicle  \\\n",
      "0                                        VeryLow                     Low   \n",
      "1                                        VeryLow                     Low   \n",
      "2                                        VeryLow                     Low   \n",
      "3                                        VeryLow                     Low   \n",
      "4                                        VeryLow                     Low   \n",
      "\n",
      "  state_and_local_government_series total_interest-bearing_debt  \\\n",
      "0                           VeryLow                     VeryLow   \n",
      "1                           VeryLow                     VeryLow   \n",
      "2                           VeryLow                     VeryLow   \n",
      "3                           VeryLow                     VeryLow   \n",
      "4                           VeryLow                     VeryLow   \n",
      "\n",
      "  total_marketable total_non-marketable treasury_bills treasury_bonds  \\\n",
      "0          VeryLow              VeryLow        VeryLow        VeryLow   \n",
      "1          VeryLow              VeryLow        VeryLow        VeryLow   \n",
      "2          VeryLow              VeryLow        VeryLow        VeryLow   \n",
      "3          VeryLow              VeryLow        VeryLow        VeryLow   \n",
      "4          VeryLow              VeryLow        VeryLow        VeryLow   \n",
      "\n",
      "  treasury_floating_rate_notes_(frn)  \\\n",
      "0                            VeryLow   \n",
      "1                            VeryLow   \n",
      "2                            VeryLow   \n",
      "3                            VeryLow   \n",
      "4                            VeryLow   \n",
      "\n",
      "  treasury_inflation-protected_securities_(tips) treasury_notes  \\\n",
      "0                                        VeryLow        VeryLow   \n",
      "1                                        VeryLow        VeryLow   \n",
      "2                                        VeryLow        VeryLow   \n",
      "3                                        VeryLow        VeryLow   \n",
      "4                                        VeryLow        VeryLow   \n",
      "\n",
      "  united_states_savings_inflation_securities united_states_savings_securities  \\\n",
      "0                                   VeryHigh                          VeryLow   \n",
      "1                                   VeryHigh                          VeryLow   \n",
      "2                                   VeryHigh                          VeryLow   \n",
      "3                                   VeryHigh                          VeryLow   \n",
      "4                                   VeryHigh                          VeryLow   \n",
      "\n",
      "  weighted_sentiment price_direction  \n",
      "0                Low              Up  \n",
      "1            VeryLow            Down  \n",
      "2               High              Up  \n",
      "3             Medium            Down  \n",
      "4            VeryLow              Up  \n",
      "\n",
      "Data types:\n",
      "volume                                            object\n",
      "federal_financing_bank                            object\n",
      "foreign_series                                     int64\n",
      "government_account_series                         object\n",
      "government_account_series_inflation_securities    object\n",
      "special_purpose_vehicle                           object\n",
      "state_and_local_government_series                 object\n",
      "total_interest-bearing_debt                       object\n",
      "total_marketable                                  object\n",
      "total_non-marketable                              object\n",
      "treasury_bills                                    object\n",
      "treasury_bonds                                    object\n",
      "treasury_floating_rate_notes_(frn)                object\n",
      "treasury_inflation-protected_securities_(tips)    object\n",
      "treasury_notes                                    object\n",
      "united_states_savings_inflation_securities        object\n",
      "united_states_savings_securities                  object\n",
      "weighted_sentiment                                object\n",
      "price_direction                                   object\n",
      "dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed dataset\n",
    "df_ready = pd.read_csv('input_data/bitcoin_sentiment_discretized.csv')\n",
    "\n",
    "print(\"Preprocessed Dataset Loaded:\")\n",
    "print(f\"  Shape: {df_ready.shape}\")\n",
    "print(f\"  Features: {df_ready.shape[1] - 1}\")\n",
    "print(f\"  Instances: {df_ready.shape[0]}\")\n",
    "print()\n",
    "\n",
    "# Display first few rows\n",
    "print(\"First 5 rows:\")\n",
    "print(df_ready.head())\n",
    "print()\n",
    "\n",
    "# Check data types\n",
    "print(\"Data types:\")\n",
    "print(df_ready.dtypes)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f76f1a",
   "metadata": {},
   "source": [
    "## Step 2: Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 2.1 Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1bba25ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLASS DISTRIBUTION ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Price Direction Distribution:\n",
      "  Down: 530 instances (49.39%)\n",
      "  Up: 543 instances (50.61%)\n",
      "\n",
      "Dataset is balanced\n",
      "Balance metric: 98.8%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze class distribution\n",
    "print(\"=\"*70)\n",
    "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "label_counts = df_ready['price_direction'].value_counts()\n",
    "label_props = df_ready['price_direction'].value_counts(normalize=True)\n",
    "\n",
    "print(\"Price Direction Distribution:\")\n",
    "for label in sorted(label_counts.index):\n",
    "    count = label_counts[label]\n",
    "    prop = label_props[label]\n",
    "    print(f\"  {label}: {count} instances ({prop*100:.2f}%)\")\n",
    "\n",
    "print()\n",
    "balance_diff = abs(label_props.iloc[0] - 0.5)\n",
    "print(f\"Dataset is {'balanced' if balance_diff < 0.1 else 'imbalanced'}\")\n",
    "print(f\"Balance metric: {(1 - balance_diff*2)*100:.1f}%\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5915fef9",
   "metadata": {},
   "source": [
    "### 2.2 Feature Distribution Analysis\n",
    "\n",
    "Note: Due to quantile-based binning, most features have approximately equal frequency distributions (~20% per bin for 5 bins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd2b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE DISTRIBUTION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Total features: 18\n",
      "\n",
      "\n",
      "volume:\n",
      "  Unique values: 5\n",
      "    High: 214 (19.9%)\n",
      "    Low: 214 (19.9%)\n",
      "    Medium: 215 (20.0%)\n",
      "    VeryHigh: 215 (20.0%)\n",
      "    VeryLow: 215 (20.0%)\n",
      "\n",
      "weighted_sentiment:\n",
      "  Unique values: 5\n",
      "    High: 214 (19.9%)\n",
      "    Low: 214 (19.9%)\n",
      "    Medium: 215 (20.0%)\n",
      "    VeryHigh: 215 (20.0%)\n",
      "    VeryLow: 215 (20.0%)\n",
      "\n",
      "federal_financing_bank:\n",
      "  Unique values: 5\n",
      "    Bank1: 131 (12.2%)\n",
      "    Bank2: 365 (34.0%)\n",
      "    Bank3: 366 (34.1%)\n",
      "    Bank4: 181 (16.9%)\n",
      "    Bank5: 30 (2.8%)\n",
      "\n",
      "foreign_series:\n",
      "  Unique values: 2\n",
      "    0: 953 (88.8%)\n",
      "    1: 120 (11.2%)\n",
      "\n",
      "total_marketable:\n",
      "  Unique values: 5\n",
      "    High: 214 (19.9%)\n",
      "    Low: 213 (19.9%)\n",
      "    Medium: 212 (19.8%)\n",
      "    VeryHigh: 192 (17.9%)\n",
      "    VeryLow: 242 (22.6%)\n"
     ]
    }
   ],
   "source": [
    "# Analyze feature distributions\n",
    "# Note: Quantile binning ensures roughly equal frequencies across bins\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE DISTRIBUTION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "feature_cols = [col for col in df_ready.columns if col != 'price_direction']\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print()\n",
    "\n",
    "# Sample key features for detailed analysis (updated - no OHLC features)\n",
    "sample_features_eda = ['volume', 'weighted_sentiment', 'federal_financing_bank', 'foreign_series', 'total_marketable']\n",
    "\n",
    "for feature in sample_features_eda:\n",
    "    if feature in df_ready.columns:\n",
    "        print(f\"\\n{feature}:\")\n",
    "        print(f\"  Unique values: {df_ready[feature].nunique()}\")\n",
    "        value_counts = df_ready[feature].value_counts().sort_index()\n",
    "        for val, count in value_counts.items():\n",
    "            print(f\"    {val}: {count} ({count/len(df_ready)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a468cf3",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Data for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2860ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation:\n",
      "  X shape: (1073, 18)\n",
      "  y shape: (1073,)\n",
      "\n",
      "  Number of features: 18\n",
      "  Number of instances: 1073\n",
      "\n",
      "Sample instance (first 5 features):\n",
      "  X[0][:5] = ['VeryHigh', 'Bank4', 1, 'VeryLow', 'VeryLow']\n",
      "  y[0] = Up\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Separate features (X) and label (y)\n",
    "X_data = df_ready.drop(columns=['price_direction']).values.tolist()\n",
    "y_data = df_ready['price_direction'].tolist()\n",
    "\n",
    "print(\"Data Preparation:\")\n",
    "print(f\"  X shape: ({len(X_data)}, {len(X_data[0])})\")\n",
    "print(f\"  y shape: ({len(y_data)},)\")\n",
    "print()\n",
    "print(f\"  Number of features: {len(X_data[0])}\")\n",
    "print(f\"  Number of instances: {len(X_data)}\")\n",
    "print()\n",
    "print(\"Sample instance (first 5 features):\")\n",
    "print(f\"  X[0][:5] = {X_data[0][:5]}\")\n",
    "print(f\"  y[0] = {y_data[0]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca7afa6",
   "metadata": {},
   "source": [
    "## Step 4: Train Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f54c2004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING RANDOM FOREST CLASSIFIER (5 Different Random States)\n",
      "======================================================================\n",
      "\n",
      "Random Forest Configuration:\n",
      "  N (number of trees): 100\n",
      "  M (best trees for final ensemble): 3\n",
      "  F (features per split): 8\n",
      "  Bootstrap sampling: Yes\n",
      "  Test set size: 33% (stratified)\n",
      "  Number of runs: 5 (different random states)\n",
      "\n",
      "Training Random Forest #1 (random_state=42)...\n",
      "  ✓ Complete!\n",
      "Training Random Forest #2 (random_state=43)...\n",
      "  ✓ Complete!\n",
      "Training Random Forest #3 (random_state=44)...\n",
      "  ✓ Complete!\n",
      "Training Random Forest #4 (random_state=45)...\n",
      "  ✓ Complete!\n",
      "Training Random Forest #5 (random_state=46)...\n",
      "  ✓ Complete!\n",
      "\n",
      "======================================================================\n",
      "✓ All 5 Random Forest instances trained successfully!\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Random Forest Classifier\n",
    "import importlib\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyRandomForestClassifier\n",
    "import math\n",
    "\n",
    "# Set hyperparameters\n",
    "N = 100  # Number of trees to train\n",
    "M = 3    # Best trees to select for final ensemble\n",
    "F = 8    # Features per split\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING RANDOM FOREST CLASSIFIER (5 Different Random States)\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "print(\"Random Forest Configuration:\")\n",
    "print(f\"  N (number of trees): {N}\")\n",
    "print(f\"  M (best trees for final ensemble): {M}\")\n",
    "print(f\"  F (features per split): {F}\")\n",
    "print(f\"  Bootstrap sampling: Yes\")\n",
    "print(f\"  Test set size: 33% (stratified)\")\n",
    "print(f\"  Number of runs: 5 (different random states)\")\n",
    "print()\n",
    "\n",
    "# Train 5 different Random Forest instances with different random states\n",
    "rf_classifiers = []\n",
    "random_states = [42, 43, 44, 45, 46]\n",
    "\n",
    "for i, rand_state in enumerate(random_states, 1):\n",
    "    print(f\"Training Random Forest #{i} (random_state={rand_state})...\")\n",
    "    \n",
    "    rf = MyRandomForestClassifier(\n",
    "        n_estimators=N,\n",
    "        n_best_trees=M,\n",
    "        max_features=F,\n",
    "        bootstrap=True,\n",
    "        random_state=rand_state,\n",
    "        test_size=0.33\n",
    "    )\n",
    "    \n",
    "    rf.fit(X_data, y_data)\n",
    "    rf_classifiers.append(rf)\n",
    "    print(f\"  ✓ Complete!\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"✓ All 5 Random Forest instances trained successfully!\")\n",
    "print(\"=\"*70)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89889c7f",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Random Forest Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f1eefc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RANDOM FOREST PERFORMANCE SUMMARY - AGGREGATE STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Evaluated 5 Random Forest instances (random_states: [42, 43, 44, 45, 46])\n",
      "\n",
      "Test Accuracy Statistics:\n",
      "  Mean:   0.5020\n",
      "  Std:    0.0221\n",
      "  Min:    0.4592\n",
      "  Max:    0.5211\n",
      "  Range:  0.0620\n",
      "\n",
      "OOB Score Statistics:\n",
      "  Mean:   0.8047\n",
      "  Std:    0.0101\n",
      "  Min:    0.7967\n",
      "  Max:    0.8245\n",
      "  Range:  0.0279\n",
      "\n",
      "Performance Analysis:\n",
      "  ~ Mean accuracy (0.5020) slightly above baseline - weak predictive power\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Random Forest performance across all 5 instances\n",
    "from mysklearn.myevaluation import confusion_matrix, accuracy_score\n",
    "\n",
    "# Collect metrics from all instances\n",
    "all_test_accs = []\n",
    "all_oob_scores = []\n",
    "\n",
    "for i, rf in enumerate(rf_classifiers, 1):\n",
    "    test_acc = rf.get_test_accuracy()\n",
    "    oob_score = rf.get_oob_score()\n",
    "    \n",
    "    if test_acc is not None:\n",
    "        all_test_accs.append(test_acc)\n",
    "    if oob_score is not None:\n",
    "        all_oob_scores.append(oob_score)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RANDOM FOREST PERFORMANCE SUMMARY - AGGREGATE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Display aggregate statistics\n",
    "print(f\"Evaluated {len(rf_classifiers)} Random Forest instances (random_states: {random_states})\")\n",
    "print()\n",
    "\n",
    "avg_test_acc = sum(all_test_accs) / len(all_test_accs)\n",
    "std_test_acc = (sum((x - avg_test_acc)**2 for x in all_test_accs) / len(all_test_accs))**0.5\n",
    "print(f\"Test Accuracy Statistics:\")\n",
    "print(f\"  Mean:   {avg_test_acc:.4f}\")\n",
    "print(f\"  Std:    {std_test_acc:.4f}\")\n",
    "print(f\"  Min:    {min(all_test_accs):.4f}\")\n",
    "print(f\"  Max:    {max(all_test_accs):.4f}\")\n",
    "print(f\"  Range:  {max(all_test_accs) - min(all_test_accs):.4f}\")\n",
    "print()\n",
    "\n",
    "avg_oob = sum(all_oob_scores) / len(all_oob_scores)\n",
    "std_oob = (sum((x - avg_oob)**2 for x in all_oob_scores) / len(all_oob_scores))**0.5\n",
    "print(f\"OOB Score Statistics:\")\n",
    "print(f\"  Mean:   {avg_oob:.4f}\")\n",
    "print(f\"  Std:    {std_oob:.4f}\")\n",
    "print(f\"  Min:    {min(all_oob_scores):.4f}\")\n",
    "print(f\"  Max:    {max(all_oob_scores):.4f}\")\n",
    "print(f\"  Range:  {max(all_oob_scores) - min(all_oob_scores):.4f}\")\n",
    "print()\n",
    "\n",
    "# Performance interpretation\n",
    "print(\"Performance Analysis:\")\n",
    "if avg_test_acc > 0.52:\n",
    "    print(f\"  ✓ Mean accuracy ({avg_test_acc:.4f}) above 50% baseline - model shows predictive signal\")\n",
    "elif avg_test_acc > 0.50:\n",
    "    print(f\"  ~ Mean accuracy ({avg_test_acc:.4f}) slightly above baseline - weak predictive power\")\n",
    "else:\n",
    "    print(f\"  ✗ Mean accuracy ({avg_test_acc:.4f}) at/below baseline - features may not predict effectively\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae13a0",
   "metadata": {},
   "source": [
    "### 5.1 Prediction Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d0c0fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE PREDICTIONS - ALL 5 INSTANCES\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "INSTANCE #1 (random_state=42) - First 10 Test Predictions\n",
      "================================================================================\n",
      "\n",
      "   1. Actual: Down | Predicted: Down | Volume:  VeryLow | Sentiment: VeryHigh | Correct: Yes\n",
      "   2. Actual:   Up | Predicted:   Up | Volume:   Medium | Sentiment:      Low | Correct: Yes\n",
      "   3. Actual: Down | Predicted: Down | Volume:  VeryLow | Sentiment:  VeryLow | Correct: Yes\n",
      "   4. Actual: Down | Predicted: Down | Volume:  VeryLow | Sentiment:     High | Correct: Yes\n",
      "   5. Actual:   Up | Predicted: Down | Volume:   Medium | Sentiment:  VeryLow | Correct:  No\n",
      "   6. Actual: Down | Predicted: Down | Volume:   Medium | Sentiment:      Low | Correct: Yes\n",
      "   7. Actual:   Up | Predicted: Down | Volume:  VeryLow | Sentiment:      Low | Correct:  No\n",
      "   8. Actual:   Up | Predicted:   Up | Volume:      Low | Sentiment:      Low | Correct: Yes\n",
      "   9. Actual:   Up | Predicted:   Up | Volume:      Low | Sentiment:      Low | Correct: Yes\n",
      "  10. Actual: Down | Predicted: Down | Volume:  VeryLow | Sentiment:  VeryLow | Correct: Yes\n",
      "\n",
      "================================================================================\n",
      "INSTANCE #2 (random_state=43) - First 10 Test Predictions\n",
      "================================================================================\n",
      "\n",
      "   1. Actual:   Up | Predicted:   Up | Volume: VeryHigh | Sentiment: VeryHigh | Correct: Yes\n",
      "   2. Actual:   Up | Predicted: Down | Volume:     High | Sentiment:  VeryLow | Correct:  No\n",
      "   3. Actual: Down | Predicted:   Up | Volume: VeryHigh | Sentiment:     High | Correct:  No\n",
      "   4. Actual: Down | Predicted:   Up | Volume: VeryHigh | Sentiment:   Medium | Correct:  No\n",
      "   5. Actual:   Up | Predicted: Down | Volume:  VeryLow | Sentiment:  VeryLow | Correct:  No\n",
      "   6. Actual: Down | Predicted:   Up | Volume: VeryHigh | Sentiment:  VeryLow | Correct:  No\n",
      "   7. Actual: Down | Predicted:   Up | Volume:     High | Sentiment: VeryHigh | Correct:  No\n",
      "   8. Actual: Down | Predicted: Down | Volume:  VeryLow | Sentiment:   Medium | Correct: Yes\n",
      "   9. Actual:   Up | Predicted:   Up | Volume:  VeryLow | Sentiment:   Medium | Correct: Yes\n",
      "  10. Actual:   Up | Predicted:   Up | Volume: VeryHigh | Sentiment:      Low | Correct: Yes\n",
      "\n",
      "================================================================================\n",
      "INSTANCE #3 (random_state=44) - First 10 Test Predictions\n",
      "================================================================================\n",
      "\n",
      "   1. Actual: Down | Predicted: Down | Volume: VeryHigh | Sentiment:   Medium | Correct: Yes\n",
      "   2. Actual: Down | Predicted: Down | Volume:   Medium | Sentiment:     High | Correct: Yes\n",
      "   3. Actual:   Up | Predicted: Down | Volume:  VeryLow | Sentiment:     High | Correct:  No\n",
      "   4. Actual:   Up | Predicted: Down | Volume:   Medium | Sentiment:      Low | Correct:  No\n",
      "   5. Actual:   Up | Predicted:   Up | Volume: VeryHigh | Sentiment:  VeryLow | Correct: Yes\n",
      "   6. Actual:   Up | Predicted:   Up | Volume:   Medium | Sentiment:     High | Correct: Yes\n",
      "   7. Actual: Down | Predicted:   Up | Volume:     High | Sentiment:  VeryLow | Correct:  No\n",
      "   8. Actual: Down | Predicted: Down | Volume:  VeryLow | Sentiment:   Medium | Correct: Yes\n",
      "   9. Actual:   Up | Predicted:   Up | Volume:      Low | Sentiment: VeryHigh | Correct: Yes\n",
      "  10. Actual: Down | Predicted: Down | Volume:   Medium | Sentiment:     High | Correct: Yes\n",
      "\n",
      "================================================================================\n",
      "INSTANCE #4 (random_state=45) - First 10 Test Predictions\n",
      "================================================================================\n",
      "\n",
      "   1. Actual:   Up | Predicted:   Up | Volume:      Low | Sentiment: VeryHigh | Correct: Yes\n",
      "   2. Actual: Down | Predicted:   Up | Volume:      Low | Sentiment: VeryHigh | Correct:  No\n",
      "   3. Actual:   Up | Predicted: Down | Volume:   Medium | Sentiment:  VeryLow | Correct:  No\n",
      "   4. Actual: Down | Predicted: Down | Volume:  VeryLow | Sentiment:      Low | Correct: Yes\n",
      "   5. Actual:   Up | Predicted:   Up | Volume:  VeryLow | Sentiment:  VeryLow | Correct: Yes\n",
      "   6. Actual: Down | Predicted: Down | Volume:      Low | Sentiment: VeryHigh | Correct: Yes\n",
      "   7. Actual: Down | Predicted: Down | Volume:  VeryLow | Sentiment: VeryHigh | Correct: Yes\n",
      "   8. Actual:   Up | Predicted: Down | Volume: VeryHigh | Sentiment:   Medium | Correct:  No\n",
      "   9. Actual:   Up | Predicted: Down | Volume:     High | Sentiment:      Low | Correct:  No\n",
      "  10. Actual:   Up | Predicted:   Up | Volume: VeryHigh | Sentiment:      Low | Correct: Yes\n",
      "\n",
      "================================================================================\n",
      "INSTANCE #5 (random_state=46) - First 10 Test Predictions\n",
      "================================================================================\n",
      "\n",
      "   1. Actual:   Up | Predicted: Down | Volume:     High | Sentiment:      Low | Correct:  No\n",
      "   2. Actual: Down | Predicted: Down | Volume:   Medium | Sentiment:  VeryLow | Correct: Yes\n",
      "   3. Actual:   Up | Predicted: Down | Volume:   Medium | Sentiment:      Low | Correct:  No\n",
      "   4. Actual:   Up | Predicted: Down | Volume:     High | Sentiment: VeryHigh | Correct:  No\n",
      "   5. Actual: Down | Predicted:   Up | Volume:   Medium | Sentiment:  VeryLow | Correct:  No\n",
      "   6. Actual:   Up | Predicted: Down | Volume:      Low | Sentiment:      Low | Correct:  No\n",
      "   7. Actual: Down | Predicted:   Up | Volume:     High | Sentiment:  VeryLow | Correct:  No\n",
      "   8. Actual:   Up | Predicted: Down | Volume: VeryHigh | Sentiment:     High | Correct:  No\n",
      "   9. Actual: Down | Predicted: Down | Volume:     High | Sentiment:      Low | Correct: Yes\n",
      "  10. Actual:   Up | Predicted: Down | Volume: VeryHigh | Sentiment:     High | Correct:  No\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Show prediction examples for all 5 Random Forest instances\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE PREDICTIONS - ALL 5 INSTANCES\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "feature_names_rf = [col for col in df_discretized_final.columns if col != 'price_direction']\n",
    "\n",
    "# Get indices for volume and weighted_sentiment\n",
    "volume_idx = feature_names_rf.index('volume')\n",
    "sentiment_idx = feature_names_rf.index('weighted_sentiment')\n",
    "\n",
    "# Show first 10 test instances for each Random Forest\n",
    "for i, rf in enumerate(rf_classifiers, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"INSTANCE #{i} (random_state={random_states[i-1]}) - First 10 Test Predictions\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print()\n",
    "    \n",
    "    y_pred_test = rf.predict(rf.X_test_internal)\n",
    "    y_true_test = rf.y_test_internal\n",
    "    \n",
    "    # Display first 10 predictions in compact format\n",
    "    for j in range(min(10, len(y_true_test))):\n",
    "        instance = rf.X_test_internal[j]\n",
    "        actual = y_true_test[j]\n",
    "        predicted = y_pred_test[j]\n",
    "        correct = 'Yes' if actual == predicted else 'No'\n",
    "        volume = instance[volume_idx]\n",
    "        sentiment = instance[sentiment_idx]\n",
    "        \n",
    "        print(f\"  {j+1:2d}. Actual: {actual:>4s} | Predicted: {predicted:>4s} | Volume: {volume:>8s} | Sentiment: {sentiment:>8s} | Correct: {correct:>3s}\")\n",
    "    \n",
    "print()\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ade1470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RANDOM FOREST HYPERPARAMETER TUNING\n",
      "================================================================================\n",
      "\n",
      "Parameter Grid:\n",
      "  N (number of trees): [10, 20, 50, 100, 200]\n",
      "  M (best trees): [3, 5, 7, 10, 15]\n",
      "  F (features per split): [2, 4, 6, 8, 12, 18]\n",
      "\n",
      "Runs per configuration: 5\n",
      "Total configurations: 150\n",
      "Total model trainings: 750\n",
      "\n",
      "Starting hyperparameter search...\n",
      "This may take several minutes...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed 10 configurations... (15.6s elapsed)\n",
      "  Completed 20 configurations... (31.1s elapsed)\n",
      "  Completed 30 configurations... (55.8s elapsed)\n",
      "  Completed 40 configurations... (84.4s elapsed)\n",
      "  Completed 50 configurations... (114.7s elapsed)\n",
      "  Completed 60 configurations... (170.9s elapsed)\n",
      "  Completed 70 configurations... (240.9s elapsed)\n",
      "  Completed 80 configurations... (313.8s elapsed)\n",
      "  Completed 90 configurations... (432.6s elapsed)\n",
      "  Completed 100 configurations... (582.8s elapsed)\n",
      "  Completed 110 configurations... (728.7s elapsed)\n",
      "  Completed 120 configurations... (960.9s elapsed)\n",
      "  Completed 130 configurations... (1226.7s elapsed)\n",
      "  Completed 140 configurations... (1500.4s elapsed)\n",
      "\n",
      "✓ Hyperparameter search complete!\n",
      "  Tested 144 valid configurations\n",
      "  Total models trained: 720\n",
      "  Time elapsed: 1614.9 seconds (26.9 minutes)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from mysklearn.myclassifiers import MyRandomForestClassifier\n",
    "from mysklearn.myevaluation import confusion_matrix, accuracy_score\n",
    "\n",
    "# Define parameter grid to search\n",
    "param_grid = {\n",
    "    'N': [10, 20, 50, 100, 200],           # Number of trees (including large values)\n",
    "    'M': [3, 5, 7, 10, 15],                # Best trees to select  \n",
    "    'F': [2, 4, 6, 8, 12, 18]              # Features per split (18 = all features)\n",
    "}\n",
    "\n",
    "# Number of runs per configuration (to handle randomness)\n",
    "n_runs = 5\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RANDOM FOREST HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(f\"Parameter Grid:\")\n",
    "print(f\"  N (number of trees): {param_grid['N']}\")\n",
    "print(f\"  M (best trees): {param_grid['M']}\")\n",
    "print(f\"  F (features per split): {param_grid['F']}\")\n",
    "print()\n",
    "print(f\"Runs per configuration: {n_runs}\")\n",
    "print(f\"Total configurations: {len(param_grid['N']) * len(param_grid['M']) * len(param_grid['F'])}\")\n",
    "print(f\"Total model trainings: {len(param_grid['N']) * len(param_grid['M']) * len(param_grid['F']) * n_runs}\")\n",
    "print()\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "print(\"Starting hyperparameter search...\")\n",
    "print(\"This may take several minutes...\")\n",
    "print()\n",
    "\n",
    "# Grid search\n",
    "total_configs = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for N in param_grid['N']:\n",
    "    for M in param_grid['M']:\n",
    "        # Skip invalid combinations (M cannot exceed N)\n",
    "        if M > N:\n",
    "            continue\n",
    "            \n",
    "        for F in param_grid['F']:\n",
    "            total_configs += 1\n",
    "            \n",
    "            # Run multiple times with different seeds\n",
    "            run_accuracies = []\n",
    "            run_oob_scores = []\n",
    "            best_conf_matrix = None\n",
    "            best_accuracy = 0\n",
    "            \n",
    "            for run in range(n_runs):\n",
    "                # Use different random seed for each run\n",
    "                random_seed = 42 + run\n",
    "                \n",
    "                try:\n",
    "                    # Train Random Forest\n",
    "                    rf = MyRandomForestClassifier(\n",
    "                        n_estimators=N,\n",
    "                        max_features=F,\n",
    "                        bootstrap=True,\n",
    "                        random_state=random_seed,\n",
    "                        test_size=0.33\n",
    "                    )\n",
    "                    \n",
    "                    rf.fit(X_data, y_data)\n",
    "                    \n",
    "                    # Get predictions\n",
    "                    y_pred = rf.predict(rf.X_test_internal)\n",
    "                    y_true = rf.y_test_internal\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    acc = accuracy_score(y_true, y_pred)\n",
    "                    oob = rf.get_oob_score()\n",
    "                    \n",
    "                    run_accuracies.append(acc)\n",
    "                    run_oob_scores.append(oob)\n",
    "                    \n",
    "                    # Keep best confusion matrix\n",
    "                    if acc > best_accuracy:\n",
    "                        best_accuracy = acc\n",
    "                        labels = sorted(list(set(y_true)))\n",
    "                        best_conf_matrix = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error with N={N}, M={M}, F={F}, run={run}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Calculate statistics\n",
    "            if run_accuracies:\n",
    "                avg_acc = sum(run_accuracies) / len(run_accuracies)\n",
    "                std_acc = (sum((x - avg_acc)**2 for x in run_accuracies) / len(run_accuracies))**0.5\n",
    "                avg_oob = sum(run_oob_scores) / len(run_oob_scores)\n",
    "                \n",
    "                results.append({\n",
    "                    'N': N,\n",
    "                    'M': M,\n",
    "                    'F': F,\n",
    "                    'avg_accuracy': avg_acc,\n",
    "                    'std_accuracy': std_acc,\n",
    "                    'min_accuracy': min(run_accuracies),\n",
    "                    'max_accuracy': max(run_accuracies),\n",
    "                    'avg_oob': avg_oob,\n",
    "                    'best_conf_matrix': best_conf_matrix,\n",
    "                    'all_accuracies': run_accuracies\n",
    "                })\n",
    "                \n",
    "                # Print progress every 10 configurations\n",
    "                if total_configs % 10 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"  Completed {total_configs} configurations... ({elapsed:.1f}s elapsed)\")\n",
    "\n",
    "elapsed_total = time.time() - start_time\n",
    "print()\n",
    "print(f\"✓ Hyperparameter search complete!\")\n",
    "print(f\"  Tested {total_configs} valid configurations\")\n",
    "print(f\"  Total models trained: {len(results) * n_runs}\")\n",
    "print(f\"  Time elapsed: {elapsed_total:.1f} seconds ({elapsed_total/60:.1f} minutes)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33c10d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HYPERPARAMETER TUNING SUMMARY - TOP 20 CONFIGURATIONS\n",
      "================================================================================\n",
      "\n",
      "  N  M  F Avg_Accuracy Std_Dev Min_Acc Max_Acc Avg_OOB\n",
      "100  3  8       0.5127  0.0211  0.4732  0.5324  0.8047\n",
      "100  5  8       0.5127  0.0211  0.4732  0.5324  0.8047\n",
      "100  7  8       0.5127  0.0211  0.4732  0.5324  0.8047\n",
      "100 10  8       0.5127  0.0211  0.4732  0.5324  0.8047\n",
      "100 15  8       0.5127  0.0211  0.4732  0.5324  0.8047\n",
      " 20  3 12       0.5110  0.0224  0.4704  0.5324  0.7788\n",
      " 20  5 12       0.5110  0.0224  0.4704  0.5324  0.7788\n",
      " 20  7 12       0.5110  0.0224  0.4704  0.5324  0.7788\n",
      " 20 10 12       0.5110  0.0224  0.4704  0.5324  0.7788\n",
      " 20 15 12       0.5110  0.0224  0.4704  0.5324  0.7788\n",
      "200  3  8       0.5099  0.0250  0.4620  0.5324  0.8089\n",
      "200  5  8       0.5099  0.0250  0.4620  0.5324  0.8089\n",
      "200  7  8       0.5099  0.0250  0.4620  0.5324  0.8089\n",
      "200 10  8       0.5099  0.0250  0.4620  0.5324  0.8089\n",
      "200 15  8       0.5099  0.0250  0.4620  0.5324  0.8089\n",
      " 10  3  6       0.5082  0.0234  0.4648  0.5324  0.7341\n",
      " 10  5  6       0.5082  0.0234  0.4648  0.5324  0.7341\n",
      " 10  7  6       0.5082  0.0234  0.4648  0.5324  0.7341\n",
      " 10 10  6       0.5082  0.0234  0.4648  0.5324  0.7341\n",
      " 50  3 12       0.5065  0.0158  0.4789  0.5239  0.8017\n",
      "\n",
      "================================================================================\n",
      "\n",
      "KEY FINDINGS:\n",
      "\n",
      "1. Best Configuration:\n",
      "   N=100, M=3, F=8\n",
      "   Test Accuracy: 0.5127 (±0.0211)\n",
      "\n",
      "2. Parameter Recommendations:\n",
      "   N (trees): 20 (avg accuracy: 0.5033)\n",
      "   M (best trees): 15 (avg accuracy: 0.5028)\n",
      "   F (features): 8 (avg accuracy: 0.5057)\n",
      "\n",
      "3. Performance Characteristics:\n",
      "   ~ Achieved 51.3% accuracy (slightly above baseline)\n",
      "   ~ Limited predictive power from available features\n",
      "\n",
      "4. Variance Analysis:\n",
      "   ~ Moderate variance (±0.0211) - acceptable\n",
      "\n",
      "5. Confusion Matrix Insights (Best Configuration):\n",
      "   Total predictions: 355\n",
      "   Correctly predicted Down: 103 / 175\n",
      "   Correctly predicted Up: 86 / 180\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Sort results by average accuracy (descending order)\n",
    "results_sorted = sorted(results, key=lambda x: x['avg_accuracy'], reverse=True)\n",
    "\n",
    "# Find best result and analyze parameter impacts\n",
    "best_result = results_sorted[0]\n",
    "\n",
    "# Analyze parameter impacts\n",
    "n_impact = {}\n",
    "m_impact = {}\n",
    "f_impact = {}\n",
    "\n",
    "for result in results:\n",
    "    N, M, F = result['N'], result['M'], result['F']\n",
    "    acc = result['avg_accuracy']\n",
    "    \n",
    "    if N not in n_impact:\n",
    "        n_impact[N] = []\n",
    "    n_impact[N].append(acc)\n",
    "    \n",
    "    if M not in m_impact:\n",
    "        m_impact[M] = []\n",
    "    m_impact[M].append(acc)\n",
    "    \n",
    "    if F not in f_impact:\n",
    "        f_impact[F] = []\n",
    "    f_impact[F].append(acc)\n",
    "\n",
    "# Create summary dataframe\n",
    "summary_data = []\n",
    "for result in results_sorted[:20]:  # Top 20 configurations\n",
    "    summary_data.append({\n",
    "        'N': result['N'],\n",
    "        'M': result['M'],\n",
    "        'F': result['F'],\n",
    "        'Avg_Accuracy': f\"{result['avg_accuracy']:.4f}\",\n",
    "        'Std_Dev': f\"{result['std_accuracy']:.4f}\",\n",
    "        'Min_Acc': f\"{result['min_accuracy']:.4f}\",\n",
    "        'Max_Acc': f\"{result['max_accuracy']:.4f}\",\n",
    "        'Avg_OOB': f\"{result['avg_oob']:.4f}\"\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING SUMMARY - TOP 20 CONFIGURATIONS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(df_summary.to_string(index=False))\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"KEY FINDINGS:\")\n",
    "print()\n",
    "print(\"1. Best Configuration:\")\n",
    "print(f\"   N={best_result['N']}, M={best_result['M']}, F={best_result['F']}\")\n",
    "print(f\"   Test Accuracy: {best_result['avg_accuracy']:.4f} (±{best_result['std_accuracy']:.4f})\")\n",
    "print()\n",
    "\n",
    "print(\"2. Parameter Recommendations:\")\n",
    "# Find best N\n",
    "best_n = max(n_impact.items(), key=lambda x: sum(x[1])/len(x[1]))\n",
    "print(f\"   N (trees): {best_n[0]} (avg accuracy: {sum(best_n[1])/len(best_n[1]):.4f})\")\n",
    "\n",
    "# Find best M\n",
    "best_m = max(m_impact.items(), key=lambda x: sum(x[1])/len(x[1]))\n",
    "print(f\"   M (best trees): {best_m[0]} (avg accuracy: {sum(best_m[1])/len(best_m[1]):.4f})\")\n",
    "\n",
    "# Find best F\n",
    "best_f = max(f_impact.items(), key=lambda x: sum(x[1])/len(x[1]))\n",
    "print(f\"   F (features): {best_f[0]} (avg accuracy: {sum(best_f[1])/len(best_f[1]):.4f})\")\n",
    "print()\n",
    "\n",
    "print(\"3. Performance Characteristics:\")\n",
    "if best_result['avg_accuracy'] > 0.52:\n",
    "    print(f\"   ✓ Achieved {best_result['avg_accuracy']:.1%} accuracy (above random baseline)\")\n",
    "    print(\"   ✓ Model shows weak but measurable predictive signal\")\n",
    "elif best_result['avg_accuracy'] > 0.50:\n",
    "    print(f\"   ~ Achieved {best_result['avg_accuracy']:.1%} accuracy (slightly above baseline)\")\n",
    "    print(\"   ~ Limited predictive power from available features\")\n",
    "else:\n",
    "    print(f\"   ✗ Achieved {best_result['avg_accuracy']:.1%} accuracy (at/below baseline)\")\n",
    "    print(\"   ✗ Features do not predict price direction effectively\")\n",
    "print()\n",
    "\n",
    "print(\"4. Variance Analysis:\")\n",
    "if best_result['std_accuracy'] < 0.02:\n",
    "    print(f\"   ✓ Low variance (±{best_result['std_accuracy']:.4f}) - stable predictions\")\n",
    "elif best_result['std_accuracy'] < 0.05:\n",
    "    print(f\"   ~ Moderate variance (±{best_result['std_accuracy']:.4f}) - acceptable\")\n",
    "else:\n",
    "    print(f\"   ✗ High variance (±{best_result['std_accuracy']:.4f}) - unstable\")\n",
    "print()\n",
    "\n",
    "print(\"5. Confusion Matrix Insights (Best Configuration):\")\n",
    "cm = best_result['best_conf_matrix']\n",
    "total = sum(sum(row) for row in cm)\n",
    "print(f\"   Total predictions: {total}\")\n",
    "print(f\"   Correctly predicted Down: {cm[0][0]} / {cm[0][0] + cm[0][1]}\")\n",
    "print(f\"   Correctly predicted Up: {cm[1][1]} / {cm[1][0] + cm[1][1]}\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478db1c",
   "metadata": {},
   "source": [
    "## Step 15: Summary of Tuning Results\n",
    "\n",
    "Create a comprehensive summary table showing simplified results for the best configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34ec3361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PARAMETER IMPACT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. Impact of N (Number of Trees):\n",
      "----------------------------------------\n",
      "  N= 10: Avg Accuracy = 0.4998 (24 configs)\n",
      "  N= 20: Avg Accuracy = 0.5033 (30 configs)\n",
      "  N= 50: Avg Accuracy = 0.5018 (30 configs)\n",
      "  N=100: Avg Accuracy = 0.5030 (30 configs)\n",
      "  N=200: Avg Accuracy = 0.5033 (30 configs)\n",
      "\n",
      "2. Impact of M (Best Trees Selected):\n",
      "----------------------------------------\n",
      "  M= 3: Avg Accuracy = 0.5022 (30 configs)\n",
      "  M= 5: Avg Accuracy = 0.5022 (30 configs)\n",
      "  M= 7: Avg Accuracy = 0.5022 (30 configs)\n",
      "  M=10: Avg Accuracy = 0.5022 (30 configs)\n",
      "  M=15: Avg Accuracy = 0.5028 (24 configs)\n",
      "\n",
      "3. Impact of F (Features per Split):\n",
      "----------------------------------------\n",
      "  F= 2: Avg Accuracy = 0.5026 (24 configs)\n",
      "  F= 4: Avg Accuracy = 0.5006 (24 configs)\n",
      "  F= 6: Avg Accuracy = 0.5015 (24 configs)\n",
      "  F= 8: Avg Accuracy = 0.5057 (24 configs)\n",
      "  F=12: Avg Accuracy = 0.5039 (24 configs)\n",
      "  F=18: Avg Accuracy = 0.4998 (24 configs)\n",
      "\n",
      "4. Overall Statistics Across All Configurations:\n",
      "----------------------------------------\n",
      "  Mean Accuracy: 0.5023\n",
      "  Best Accuracy: 0.5127\n",
      "  Worst Accuracy: 0.4890\n",
      "  Std Dev: 0.0048\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PARAMETER IMPACT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Analyze impact of N (number of trees)\n",
    "print(\"1. Impact of N (Number of Trees):\")\n",
    "print(\"-\" * 40)\n",
    "n_impact = {}\n",
    "for result in results:\n",
    "    n = result['N']\n",
    "    if n not in n_impact:\n",
    "        n_impact[n] = []\n",
    "    n_impact[n].append(result['avg_accuracy'])\n",
    "\n",
    "for n in sorted(n_impact.keys()):\n",
    "    avg = sum(n_impact[n]) / len(n_impact[n])\n",
    "    print(f\"  N={n:3d}: Avg Accuracy = {avg:.4f} ({len(n_impact[n])} configs)\")\n",
    "print()\n",
    "\n",
    "# Analyze impact of M (best trees selected)\n",
    "print(\"2. Impact of M (Best Trees Selected):\")\n",
    "print(\"-\" * 40)\n",
    "m_impact = {}\n",
    "for result in results:\n",
    "    m = result['M']\n",
    "    if m not in m_impact:\n",
    "        m_impact[m] = []\n",
    "    m_impact[m].append(result['avg_accuracy'])\n",
    "\n",
    "for m in sorted(m_impact.keys()):\n",
    "    avg = sum(m_impact[m]) / len(m_impact[m])\n",
    "    print(f\"  M={m:2d}: Avg Accuracy = {avg:.4f} ({len(m_impact[m])} configs)\")\n",
    "print()\n",
    "\n",
    "# Analyze impact of F (features per split)\n",
    "print(\"3. Impact of F (Features per Split):\")\n",
    "print(\"-\" * 40)\n",
    "f_impact = {}\n",
    "for result in results:\n",
    "    f = result['F']\n",
    "    if f not in f_impact:\n",
    "        f_impact[f] = []\n",
    "    f_impact[f].append(result['avg_accuracy'])\n",
    "\n",
    "for f in sorted(f_impact.keys()):\n",
    "    avg = sum(f_impact[f]) / len(f_impact[f])\n",
    "    print(f\"  F={f:2d}: Avg Accuracy = {avg:.4f} ({len(f_impact[f])} configs)\")\n",
    "print()\n",
    "\n",
    "# Overall statistics\n",
    "all_accuracies = [r['avg_accuracy'] for r in results]\n",
    "print(\"4. Overall Statistics Across All Configurations:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Mean Accuracy: {sum(all_accuracies)/len(all_accuracies):.4f}\")\n",
    "print(f\"  Best Accuracy: {max(all_accuracies):.4f}\")\n",
    "print(f\"  Worst Accuracy: {min(all_accuracies):.4f}\")\n",
    "print(f\"  Std Dev: {(sum((x - sum(all_accuracies)/len(all_accuracies))**2 for x in all_accuracies) / len(all_accuracies))**0.5:.4f}\")\n",
    "print()\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce65b66f",
   "metadata": {},
   "source": [
    "## Step 14: Parameter Impact Analysis\n",
    "\n",
    "Analyze how each parameter affects performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e10089f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOP 10 HYPERPARAMETER CONFIGURATIONS\n",
      "================================================================================\n",
      "\n",
      "1. N=100, M=3, F=8\n",
      "   Average Accuracy: 0.5127 (±0.0211)\n",
      "   Range: [0.4732, 0.5324]\n",
      "   Average OOB Score: 0.8047\n",
      "   Individual runs: ['0.5099', '0.5268', '0.5211', '0.5324', '0.4732']\n",
      "\n",
      "2. N=100, M=5, F=8\n",
      "   Average Accuracy: 0.5127 (±0.0211)\n",
      "   Range: [0.4732, 0.5324]\n",
      "   Average OOB Score: 0.8047\n",
      "   Individual runs: ['0.5099', '0.5268', '0.5211', '0.5324', '0.4732']\n",
      "\n",
      "3. N=100, M=7, F=8\n",
      "   Average Accuracy: 0.5127 (±0.0211)\n",
      "   Range: [0.4732, 0.5324]\n",
      "   Average OOB Score: 0.8047\n",
      "   Individual runs: ['0.5099', '0.5268', '0.5211', '0.5324', '0.4732']\n",
      "\n",
      "4. N=100, M=10, F=8\n",
      "   Average Accuracy: 0.5127 (±0.0211)\n",
      "   Range: [0.4732, 0.5324]\n",
      "   Average OOB Score: 0.8047\n",
      "   Individual runs: ['0.5099', '0.5268', '0.5211', '0.5324', '0.4732']\n",
      "\n",
      "5. N=100, M=15, F=8\n",
      "   Average Accuracy: 0.5127 (±0.0211)\n",
      "   Range: [0.4732, 0.5324]\n",
      "   Average OOB Score: 0.8047\n",
      "   Individual runs: ['0.5099', '0.5268', '0.5211', '0.5324', '0.4732']\n",
      "\n",
      "6. N=20, M=3, F=12\n",
      "   Average Accuracy: 0.5110 (±0.0224)\n",
      "   Range: [0.4704, 0.5324]\n",
      "   Average OOB Score: 0.7788\n",
      "   Individual runs: ['0.5324', '0.5042', '0.5268', '0.5211', '0.4704']\n",
      "\n",
      "7. N=20, M=5, F=12\n",
      "   Average Accuracy: 0.5110 (±0.0224)\n",
      "   Range: [0.4704, 0.5324]\n",
      "   Average OOB Score: 0.7788\n",
      "   Individual runs: ['0.5324', '0.5042', '0.5268', '0.5211', '0.4704']\n",
      "\n",
      "8. N=20, M=7, F=12\n",
      "   Average Accuracy: 0.5110 (±0.0224)\n",
      "   Range: [0.4704, 0.5324]\n",
      "   Average OOB Score: 0.7788\n",
      "   Individual runs: ['0.5324', '0.5042', '0.5268', '0.5211', '0.4704']\n",
      "\n",
      "9. N=20, M=10, F=12\n",
      "   Average Accuracy: 0.5110 (±0.0224)\n",
      "   Range: [0.4704, 0.5324]\n",
      "   Average OOB Score: 0.7788\n",
      "   Individual runs: ['0.5324', '0.5042', '0.5268', '0.5211', '0.4704']\n",
      "\n",
      "10. N=20, M=15, F=12\n",
      "   Average Accuracy: 0.5110 (±0.0224)\n",
      "   Range: [0.4704, 0.5324]\n",
      "   Average OOB Score: 0.7788\n",
      "   Individual runs: ['0.5324', '0.5042', '0.5268', '0.5211', '0.4704']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "BEST CONFIGURATION:\n",
      "  N (number of trees): 100\n",
      "  M (best trees): 3\n",
      "  F (features per split): 8\n",
      "\n",
      "Performance Metrics:\n",
      "  Average Test Accuracy: 0.5127 (±0.0211)\n",
      "  Best Single Run: 0.5324\n",
      "  Worst Single Run: 0.4732\n",
      "  Average OOB Score: 0.8047\n",
      "\n",
      "Best Run Confusion Matrix:\n",
      "  Actual↓ / Predicted→\n",
      "         Down      Up\n",
      "  Down    103      72\n",
      "  Up       94      86\n",
      "\n",
      "Per-Class Performance (Best Run):\n",
      "  Class 'Down':\n",
      "    Precision: 0.5228\n",
      "    Recall:    0.5886\n",
      "    F1-Score:  0.5538\n",
      "\n",
      "  Class 'Up':\n",
      "    Precision: 0.5443\n",
      "    Recall:    0.4778\n",
      "    F1-Score:  0.5089\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Sort results by average accuracy\n",
    "results_sorted = sorted(results, key=lambda x: x['avg_accuracy'], reverse=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TOP 10 HYPERPARAMETER CONFIGURATIONS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Display top 10 results\n",
    "for i, result in enumerate(results_sorted[:10], 1):\n",
    "    print(f\"{i}. N={result['N']}, M={result['M']}, F={result['F']}\")\n",
    "    print(f\"   Average Accuracy: {result['avg_accuracy']:.4f} (±{result['std_accuracy']:.4f})\")\n",
    "    print(f\"   Range: [{result['min_accuracy']:.4f}, {result['max_accuracy']:.4f}]\")\n",
    "    print(f\"   Average OOB Score: {result['avg_oob']:.4f}\")\n",
    "    print(f\"   Individual runs: {[f'{x:.4f}' for x in result['all_accuracies']]}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Find best configuration\n",
    "best_result = results_sorted[0]\n",
    "print(\"BEST CONFIGURATION:\")\n",
    "print(f\"  N (number of trees): {best_result['N']}\")\n",
    "print(f\"  M (best trees): {best_result['M']}\")\n",
    "print(f\"  F (features per split): {best_result['F']}\")\n",
    "print()\n",
    "print(f\"Performance Metrics:\")\n",
    "print(f\"  Average Test Accuracy: {best_result['avg_accuracy']:.4f} (±{best_result['std_accuracy']:.4f})\")\n",
    "print(f\"  Best Single Run: {best_result['max_accuracy']:.4f}\")\n",
    "print(f\"  Worst Single Run: {best_result['min_accuracy']:.4f}\")\n",
    "print(f\"  Average OOB Score: {best_result['avg_oob']:.4f}\")\n",
    "print()\n",
    "\n",
    "# Display best confusion matrix\n",
    "print(\"Best Run Confusion Matrix:\")\n",
    "print(f\"  Actual↓ / Predicted→\")\n",
    "print(f\"         Down      Up\")\n",
    "cm = best_result['best_conf_matrix']\n",
    "print(f\"  Down   {cm[0][0]:4d}    {cm[0][1]:4d}\")\n",
    "print(f\"  Up     {cm[1][0]:4d}    {cm[1][1]:4d}\")\n",
    "print()\n",
    "\n",
    "# Calculate per-class metrics for best result\n",
    "tp_down = cm[0][0]\n",
    "fp_down = cm[1][0]\n",
    "fn_down = cm[0][1]\n",
    "tn_down = cm[1][1]\n",
    "\n",
    "tp_up = cm[1][1]\n",
    "fp_up = cm[0][1]\n",
    "fn_up = cm[1][0]\n",
    "tn_up = cm[0][0]\n",
    "\n",
    "prec_down = tp_down / (tp_down + fp_down) if (tp_down + fp_down) > 0 else 0\n",
    "rec_down = tp_down / (tp_down + fn_down) if (tp_down + fn_down) > 0 else 0\n",
    "f1_down = 2 * prec_down * rec_down / (prec_down + rec_down) if (prec_down + rec_down) > 0 else 0\n",
    "\n",
    "prec_up = tp_up / (tp_up + fp_up) if (tp_up + fp_up) > 0 else 0\n",
    "rec_up = tp_up / (tp_up + fn_up) if (tp_up + fn_up) > 0 else 0\n",
    "f1_up = 2 * prec_up * rec_up / (prec_up + rec_up) if (prec_up + rec_up) > 0 else 0\n",
    "\n",
    "print(\"Per-Class Performance (Best Run):\")\n",
    "print(f\"  Class 'Down':\")\n",
    "print(f\"    Precision: {prec_down:.4f}\")\n",
    "print(f\"    Recall:    {rec_down:.4f}\")\n",
    "print(f\"    F1-Score:  {f1_down:.4f}\")\n",
    "print()\n",
    "print(f\"  Class 'Up':\")\n",
    "print(f\"    Precision: {prec_up:.4f}\")\n",
    "print(f\"    Recall:    {rec_up:.4f}\")\n",
    "print(f\"    F1-Score:  {f1_up:.4f}\")\n",
    "print()\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
